\chapter{Personalized SimRank Properties}
\label{appendix:PersonalizedSimRankProperties}

In this appendix, we prove some of the important properties of the Personalized SimRank System introduced in Section \ref{subsection:PersonalizedSimRank}.\\

\noindent
\textsc{Proposition 0} : The sequence $S_k(\alpha,\beta)$ converges for every node pair $\alpha,\beta \in V$

\noindent
\textsc{Proof} :
It follows from the monotonocity of the sequence $S_k(\alpha,\beta)$ : 
\begin{equation}
0 \leq S_k(\alpha,\beta) \leq S_{k+1}(\alpha,\beta) \leq 1 \mbox{ for all } \alpha,\beta \in V, k \geq 0
\end{equation}

This says for every $\alpha,\beta$, the sequence $\{S_k(\alpha,\beta)\}$ is bounded and non-decreasing as $k$ increases and hence each sequence $\{S_k(\alpha,\beta)\}$ converges to a limit $S(\alpha,\beta) \in [0,1]$.\\

\noindent
\textsc{Proposition 1} : The system of equations of Personalized SimRank has a solution, which is constructed by the iterative fixed point solution defined in Section \ref{subsection:PersonalizedSimRank}.

\noindent
\textsc{Proof} :
The unique solution is actually constructed in Section \ref{subsection:PersonalizedSimRank}, and the correctness of the iterative algorithm follows. 

Using \textsc{Proposition 0} and the fact that limit of a sum is the sum of the limits, we have :
\begin{equation} \label{eq:PersonalizedWeightedSimRankLimit}
{\footnotesize
S(\alpha,\beta) =
\left\{ \begin{array}{rl}
1 & \mbox{if } \alpha=\beta\\
InitStore[(\alpha,\beta)] & \mbox{if } (\alpha,\beta) \in InitStore\\
\frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot S(I_i(\alpha),I_j(\beta)) & \mbox{otherwise }
\end{array}\right.
}
\end{equation}

which shows that the limits $S(\ast, \ast)$ satisfy the simrank equations. \\

\noindent
\textsc{Proposition 2} : The system of equations of Personalized SimRank has a unique solution.

\noindent
\textsc{Proof} :
Suppose $s_1(\ast,\ast)$ and $s_2(\ast,\ast)$ are two solutions to the $|V|^2$ Personalized SimRank equations.
For all $\alpha,\beta \in V$, let $\delta(\alpha,\beta) = s_1(\alpha,\beta)-s_2(\alpha,\beta)$ be their difference. 
Let $M = max_{(\alpha,\beta)}|\delta(\alpha,\beta)|$ be the maximum absolute value of any difference.
We need to show that $M=0$.
Let $|\delta(\alpha,\beta)| = M$ for some $\alpha,\beta \in V$. Certainly $M=0$ if $\alpha,\beta$ or if $(\alpha,\beta)\in InitStore$ or if $I(\alpha) = \emptyset$ or $I(\beta)=\emptyset$.
Otherwise, $s_1(\alpha,\beta)$ and $s_2(\alpha,\beta)$ are the weighted average scores of their in-neighbours. 
That is, from equation \ref{eq:PersonalizedWeightedSimRank},

\begin{align*}
s_1(\alpha,\beta) =& \frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot s_1(I_i(\alpha),I_j(\beta))\\
s_2(\alpha,\beta) =& \frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot s_2(I_i(\alpha),I_j(\beta))\\
\end{align*}

\noindent
In terms of $\delta(\alpha,\beta)$,
\begin{align*}
\delta(\alpha,\beta) =& s_1(\alpha,\beta) - s_2(\alpha,\beta)\\
=& \frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot [s_1(I_i(\alpha),I_j(\beta)) - s_2(I_i(\alpha),I_j(\beta))]\\
=&\frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot \delta(I_i(\alpha),I_j(\beta))
\end{align*}

\noindent
Thus, 
\begin{align*}
M =& |\delta(\alpha,\beta)|\\
=& \left| \frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot \delta(I_i(\alpha),I_j(\beta))\right|\\
\leq& \frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot \left| \delta(I_i(\alpha),I_j(\beta))\right|\\
\leq&\frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot M\\
=& CM
\end{align*}

\noindent
Since $0 < C < 1$, it follows that $M=0$\\

\noindent
\textsc{Proposition 3} : \textit{The difference between SimRank theoretical and iterative similarity scores decreases exponentially in the number of iterations for every pair of nodes. 
Precisely, for every iteration number $t = 0, 1, 2, \ldots$ and for every two nodes $\alpha,\beta$, the following estimate holds:}
\begin{align}
s(\alpha,\beta) - S_{t}(\alpha,\beta) \leq C^{t+1} \label{eq:convergenceSimRankPersonalized}
\end{align}

\noindent
\textsc{Proof} :
If $\alpha=\beta$ then $s(\alpha,\alpha)=S_k(\alpha,\alpha)=1$ by definition for every $k=0,1,2,\ldots$, the left side of \ref{eq:convergenceSimRankPersonalized} is 0 and thus \ref{eq:convergenceSimRankPersonalized} obviously holds.
Similarly, if $I(\alpha) = \emptyset$ or $I(\beta) = \emptyset$ then by definition $s(\alpha,\beta)=S_k(\alpha,\beta)=0$.
Also, lets consider the case when $(\alpha,\beta)\in InitStore$. In this case, $s(\alpha,\beta)=S_k(\alpha,\beta)=InitStore[(\alpha,\beta)]$ and hence \ref{eq:convergenceSimRankPersonalized} always holds.

For the general case of $\alpha \neq \beta$, $I(\alpha) \neq \emptyset$, $I(\beta) \neq \emptyset$ and $(\alpha,\beta) \in InitStore$, the proof is organized by mathematical induction.

\textbf{Induction Basis} : Let us prove that \ref{eq:convergenceSimRankPersonalized} holds for $k=0$ i.e. for every two nodes $\alpha,\beta$ :
\begin{align}
s(\alpha,\beta) - S_0(\alpha,\beta) \leq C \label{eq:baseCase}
\end{align}

Since $\alpha \neq \beta$, $I(\alpha) \neq \emptyset$, $I(\beta) \neq \emptyset$ and $(\alpha,\beta) \in InitStore$, then $S_0(\alpha,\beta)=0$. 
\begin{align*}
s(\alpha,\beta) - S_0(\alpha,\beta) &= s(\alpha,\beta)\\
&=\frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot \underbrace{s(I_i(\alpha),I_j(\beta))}_{\leq 1}\\
&=\frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(a)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \\
&\leq C
\end{align*}
which proves \ref{eq:baseCase}

\textbf{Inductive Step} : Provided that \ref{eq:convergenceSimRankPersonalized} holds for a given $k$ for all node pairs, let us prove that \ref{eq:convergenceSimRankPersonalized} holds for $(k+1)$ as well:
\begin{align*}
s(\alpha,\beta) - S_{k+1}(\alpha,\beta) &= \frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot s(I_i(\alpha),I_j(\beta))\\
&-\frac{C}{W_{I}(\alpha)W_{I}(\beta)} \sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot S_k(I_i(\alpha),I_j(\beta)) \\
&=\frac{C}{W_{I}(\alpha)W_{I}(\beta)}\\
&\sum_{i=1}^{|I(\alpha)|} \sum_{j=1}^{|I(\beta)|} w(I_i(\alpha),\alpha) \cdot w(I_j(\beta),\beta) \cdot \underbrace{[s(I_i(\alpha),I_j(\beta))-S_k(I_i(\alpha),I_j(\beta))]}_{\leq C^{k+1} \mbox{ by inductive hypothesis}}\\
&\leq C^{(k+1)+1}
\end{align*}

The latter finally proves \ref{eq:convergenceSimRankPersonalized}.


