\documentclass[10pt]{beamer}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{color}
\usepackage[numbers]{natbib}
\usepackage{framed}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{chngpage}
\usepackage{caption}
\usepackage{tikz}
\usepackage{mdwlist}
\usepackage{algorithm,algorithmic,amsmath}



\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO:} #1}}
\newcommand{\setD}{\ensuremath{D} }
\newcommand{\setW}{\ensuremath{V} }
\newcommand{\setC}{\ensuremath{C} }
\newcommand{\matD}{\ensuremath{\mathrm{D}} }
\newcommand{\matW}{\ensuremath{\mathrm{W}} }
\newcommand{\matC}{\ensuremath{\mathrm{C}} }
\newcommand{\vecdi}[1]{\ensuremath{\mathrm{v}^{D}_{#1}}}
\newcommand{\vecwi}[1]{\ensuremath{\mathrm{v}^{W}_{#1}}}
\newcommand{\vecci}[1]{\ensuremath{\mathrm{v}^{C}_{#1}}}
\newcommand{\con}{$(w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})$}
\newcommand{\wgt}[1]{\ensuremath{\lambda_{#1}}}
\newcommand{\traindata}{\ensuremath{\mathcal{T}}}
\newcommand{\db}{\ensuremath{\mathcal{D}} }
\newcommand{\para}[1]{\noindent\textbf{%\fontsize{14}{15}\selectfont 
#1}} %{\paragraph{#1}}
\newcommand{\highest}[1]{\textbf{#1}}

\usetheme{Madrid}
%\usetheme{umbc1}
\title[{Distributed Document Representations for Multi-Label Document Categorization}]{Learning Distributed Document Representations for Multi-Label Document Categorization}
\author{\textbf{Nitish Gupta}}
\date{May 16, 2015}
\institute[IITK]{B.Tech - M.Tech Dual Degree\\
\vspace{.2cm}Thesis Defense\\ 
\vspace{.2cm}Electrical Engineering\\
\vspace{.2cm}IIT Kanpur }
\begin{document}
\setbeamercovered{clear}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}
\titlepage
%\begin{figure}[ht]
%\begin{center}
%\includegraphics[width=1.75cm]{mips}
%\epsfig{file=iitk_logo.eps,width=1.5cm}
%\end{center}
%\end{figure}
\begin{figure}[ht]
\begin{center}
\includegraphics[width=1.75cm]{iitk_logo-eps-converted-to.pdf}
%\epsfig{file=iitk_logo.eps,width=1.5in}
\end{center}
\end{figure}
%\begin{figure}
%\begin{center}
%\hspace{-.0cm}
%\vspace{2cm}\includegraphics[width=1.25cm]{mips-eps-converted-to.pdf}
%\end{center}
%\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Outline}
\begin{enumerate}
	\vfill\item Multi-Label Document Categorization
	\vfill\item Related Work
	\begin{itemize}
	  \vfill\item Text Representations
	  \vfill\item Learning Algorithms
	\end{itemize}  
	\vfill\item Distributed Word Representations
	\vfill\item Learning Distributed Document Represenations
	\vfill\item Document Cateogorization Algorithm
	\vfill\item Results
	\vfill\item Conclusion and Future Work
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Intoduction to Multi-Label Document Categorization}
\begin{itemize}
	\vfill\item<1-> Text Documents usually belong to more than one conceptual class. \\For E.g. an article on Music Piracy 
	\vfill\item<2-> Task of assigning documents to one or more predefined categories is called \emph{Multi-Label Document Categorization}
	\vfill\item<3-> Wide range real-world applications :
	\begin{itemize} 
	  \vfill\item<3-> Web-page tagging
	  \vfill\item<3-> Medical Patient Record Management
	  \vfill\item<3-> Wikipedia Article Management
	  \vfill\item<3-> Document Recommendation etc.
	\end{itemize} 
	\vfill\item<4-> Multi-label classification belongs to a general class of supervised learning algorithms where : 
	\begin{itemize}
	  \vfill\item<5-> Training instances in the form of document-category pairs are used to learn a classifier $\mathcal{H}$ 
	  \vfill\item<6-> Learned classifier $\mathcal{H}$ is used to assign categories to new test documents
	\end{itemize}
\end{itemize}
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Introduction to Multi-Label Document Categorization}
%\onslide<1->{ \textbf{Problem Statement}, } \\
\vfill\onslide<1->{ Given, }
\begin{itemize} 
	\vfill\item<1-> A set of documents $D = \{d_{1}, \ldots, d_{|D|}\}$
	\vfill\item<2-> A set of categories $C = \{c_{1}, \ldots, c_{|C|}\}$
	\vfill\item<3-> Training data for $n$ {\small ($n < |D|$)} documents, $\traindata = \{l_{d_{1}}, \ldots, l_{d_{n}}\}$ 
\onslide<4->{   \\ Each label vector $l_{d_{i}} \in \{0,1\}^{|C|}$ denotes relevance of categories to the document $d_{i}$ }
\end{itemize} 
\vfill
\onslide<5->{
	Example :
	\begin{table}[h!]
	\tabcolsep=0.1cm
	\tiny
	\begin{center}
	\begin{tabular}{c@{\hskip5mm} c c c c c c}
	\toprule
	\textbf{Documents}	&	\textbf{Sports} & \textbf{Music} & \textbf{Arts} & \textbf{Technology}  & \textbf{Literature} & \textbf{Politics}\\
	\cmidrule{1-1}
	\cmidrule{2-7}
	$d_{1}$ & 0 & 0 & 1 & 0 & 1 & 0\\
	$d_{2}$ & 0 & 1 & 1 & 0 & 0 & 1\\
	$d_{3}$ & 1 & 0 & 0 & 1 & 0 & 1\\
	$d_{4}$ & x & x & x & x & x & x\\
	$d_{5}$ & x & x & x & x & x & x\\
	\bottomrule         
	\end{tabular}
	\end{center}
	\end{table}
}
\vfill
\onslide<6->{
Using $\traindata$, $D$ and $C$ the learning algorithm learns a multi-label classifier $\mathcal{H}$ to estimate category label vectors, $l_{d_{j}}$ {\small $(j>n)$} for the test documents. 
}
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Introduction to Multi-Label Document Categorization}
\vfill\onslide<1->{ Document Categorization task has the following two components : }
\begin{enumerate}
	\vfill\item<2-> \emph{Learning Document Representations} : Representing text documents using numerical vectors that are inputs to the multi-label classifier $\mathcal{H}$
	\begin{itemize}
	 	\vfill\item<3-> Each document $d_{i} \in D$ is represented using a vector $v_{d_{i}} \in \mathbb{R}^{k}$ \\
	 	\vfill\item<4->	Vectors ($v_{d_{i}}$) should encode the semantic content of the documents 
	 	\vfill\item<5-> Encoding documents in a $k$-dimensional space using such represenation is called the \emph{Vector Space Model}
	 	\vfill\item<6-> The complete document set $D$ can be represented by a document representation matrix $\matD \in \mathbb{R}^{k \times |D|}$
	\end{itemize}
\suspend{enumerate} \vfill
\onslide<7->{	
\emph{ In this thesis, we focus on learning efficient document representations, $\matD$}	}
\vfill
	\resume{enumerate}
	\vfill\item<8-> \emph{Learning Algorithm} : Algorithm to learn the multi-label classifier $\mathcal{H}$
\end{enumerate}\vfill 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Background on Learning Algorithms}
\vfill
%\onslide<1->{ Learning algorithms can be classified into two classes :  }
\begin{enumerate}
	\vfill\item<2-> \emph{Learning Multiple Binary Classifiers} : \\
	\onslide<3->{ {\footnotesize	Algorithms that treat each category assignment independently and learn multiple binary classifiers, one for each category, to make the category assignments} 	} 
	\begin{itemize}
		\vfill\item<4-> Logistic Regression
		\vfill\item<5-> Support Vector Machines (SVM)
		\vfill\item<6-> Neural Networks
		\vfill\item<7-> Naive Bayes
	\end{itemize}

	\vfill\item<8-> \emph{Learning Single Joint Classifier} : \\
	\onslide<9->{ {\footnotesize	Algorithms that jointly assign all the categories to a document $d_{i}$, i.e. estimate the complete label vector $l_{d_{i}}$ using a single classifier} 	}
	\begin{itemize}
		\vfill\item<10-> k-Nearest Neighbor (k-NN)
		\vfill\item<11-> Linear Least Square Fit
		\vfill\item<12-> Decision Trees
		\vfill\item<13-> Generative Probabilistic Models
	\end{itemize}
\end{enumerate}
%\onslide<14->{ Learning a single joint classifier is usually better as it is able to exploit the category correlations }
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Background on Text Representation}
\vfill
\onslide<1->{
\textbf{Bag of Words Model} }
%Most common method to learn vector representations for a set of documents is the \emph{Bag of Words (BOW)} model 	}
\begin{itemize}
	\vfill\item<2-> Document $d_{i}$ represented by $v_{d_{i}} \in \mathbb{R}^{|V|}$
	\vfill\item<3-> Each element in $v_{d_{i}}$ denotes presence/absence of each word
	\vfill\item<4-> Weighing techniques employed to give importance to important terms
	\begin{itemize}
		\vfill\item<5-> Term Frequency (\emph{tf} )
		\vfill\item<5-> Inverse Document Frequency (\emph{idf} )
		\vfill\item<5-> Term Frequency - Inverse Document Frequency (\emph{tf-idf} ) : \emph{tf} $\times$\emph{idf}
	\end{itemize}
\end{itemize}
\vfill
\onslide<6->{
\textbf{Drawbacks of the Bag-of-Words model} }
\begin{itemize}
	\vfill\item<7-> High-dimensionality
	\vfill\item<8-> Sparsity
	\vfill\item<9-> Inability to encode word contexts
	\vfill\item<10-> Ignores word order
\end{itemize}
\vfill
% \todo{feature selection. Word Embeddings. Our document embedding. Our docu cate. Datasets. Results End}
% \vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Background on Feature Selection / Dimensionality Reduction}
\vfill
\onslide<1-> {
Techniques to deal with sparsity and high-dimensionality in BOW }
\begin{itemize}
	\vfill\item<2-> Information Gain \\
	\onslide<2->{ { \scriptsize
	\begin{equation}
	G(t) = -\sum_{i=1}^{|C|} P(c_{i})\log P(c_{i}) + P(t)\sum_{i=1}^{|C|} P(c_{i}|t)\log P(c_{i}|t) + P(\sim t)\sum_{i=1}^{|C|} P(c_{i}|\sim t)\log P(c_{i}|\sim t)
	\end{equation}
	} }
	\vfill\item<3-> Mutual Information \\
	\onslide<3->{ { \scriptsize
	\begin{equation}
	I(t,c) = \log \frac{P(t \wedge c)}{P(t) \times P(c)} \text{,}\qquad I_{avg}(t) = \sum_{i=1}^{|C|} P(c_{i})I(t,c_{i})
	\end{equation}
	} }
	\vfill\item<4-> Latent Semantic Indexing (LSI) \\
	\onslide<4->{ { \scriptsize
	\begin{equation}
	X = T S D^{T}
	\end{equation}
	} }
\end{itemize}
\vfill
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Distributed Word Representations}
%\vfill
\onslide<1->{
Representating each word $w_{i}$ using vector $v_{w_{i}} \in \mathbb{R}^{k}$ ($k \in [50, 300]$) } \\ \vspace{0.5cm}
\onslide<2->{
\textbf{ Need for Distributed Word Representations} } 
\begin{itemize}
	\vfill\item<3-> Curse of Dimensionality
	\begin{itemize}
		\vfill\item<4-> One-hot representations grow with the size of vocabulary
		\vfill\item<5-> Parameters in language modelling grow exponentially with the size of vocabulary
	\end{itemize}
	\vfill\item<6-> No Word Similarity Measure
	\begin{itemize}
		\vfill\item<7-> One-hot representations are orthogonal representations
		\vfill\item<8-> Cannot capture semantic similarity between words
	\end{itemize}
\end{itemize}
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Neural Probabilistic Language Model}
\vfill
\onslide<1->{
\citet{bengio2003neural} developed \emph{Neural Probabilistic Language Model (NPLM)} to learn}%learn distributed word vectors and a probability function that uses these vectors to learn a statistical model of language	}
\begin{enumerate}
\vfill\item<2-> Distributed word vectors
\vfill\item<3-> A probability function that, using these word vectors, learns a statistical model of language 
\end{enumerate}

\begin{columns}[T]
 \begin{column}{.5\textwidth}
 	\centering
 	\onslide<4->{
 	\includegraphics[scale=0.15]{../figs/bengio_nn.png} }

 \end{column}

 \begin{column}{.5\textwidth}
 	\centering
 	{\footnotesize
 	\vfill \onslide<5->{
 	\begin{equation}
    P(w_{t} | w_{1}^{t-1}) \approx P(w_{t} | w_{t-n+1}^{t-1})
    \end{equation} }
    \vfill \onslide<6->{
    \begin{equation}
	y = b + U tanh(d + Hx) \text{,} \quad y \in \mathbb{R}^{|V|}
	\end{equation} }
	\vfill \onslide<7->{
	\begin{equation}
	P(w_{t} = i | w_{t-1}, \ldots, w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}e^{y_{i}}}
	\end{equation} }
	}
 \end{column}
\end{columns}

\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Log-Linear Models}
\vfill
\onslide<1->{
\emph{Log-Linear Models} for learning distributed word vectors are proposed in \citet{mikolov2013efficient}. These models use word vectors to predict other words in the context.	 }
% \begin{itemize}
% \vfill\item<2-> do not build a language model hence also consider future words in contexts
% \vfill\item<3-> show that the word vectors capture the semantic similarity between words
% \end{itemize}
\vfill
\begin{enumerate}
	\vfill\item<2-> Continuous Bag-of-Words Model 
	\begin{columns}[T]

	 \begin{column}{.5\textwidth}
	 	\centering
	 	\onslide<2->{
	 	\includegraphics[scale=0.12]{../figs/mikolov_cbow.png} }
	 \end{column}
	 \begin{column}{.5\textwidth}
	 	\centering
	 	{\footnotesize
	 	\onslide<3->{
	 	\begin{equation}
	    h = w_{t-k} + \ldots + w_{t-1} + w_{t+1} + \dots + w_{t+k}
	    \end{equation} }
	    \onslide<4->{
	    \begin{equation}
		y = b + Uh \text{,} \quad y \in \mathbb{R}^{|V|}
		\end{equation} }
		\onslide<5->{
		\begin{equation}
		P(w_{t}|w_{t-k}, \ldots, w_{t+k}) = \frac{e^{y_{w_t}}}{\sum_{i} e^{y_{i}}}
		\end{equation} }
		}
	 \end{column}
	\end{columns}

	\vfill\item<6-> Skip-Gram Model
	\begin{columns}[T]

	 \begin{column}{.5\textwidth}
	 	\centering
	 	\onslide<6->{
	 	\includegraphics[scale=0.12]{../figs/mikolov_skip.png} }
	 \end{column}
	 \begin{column}{.5\textwidth}
	 	\centering
	 	\vfill
	 	{\footnotesize
	 	\onslide<7->{
	 	\begin{equation}
	    P(w_{t+j}|w_{t}) = \frac{e^{(v_{w_{t}} \cdot v_{w_{t+j}} )} }{\sum_{i} e^{(v_{w_{t}} \cdot v_{w_{i}})} }
	    \end{equation} }
	    }
	 \end{column}
	\end{columns}
\end{enumerate}

\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%

\begin{frame}{Distributed Document Representations}
\vfill \onslide<1->{
\textbf{Motivation for learning distributed document representations} }
\begin{enumerate}
	\vfill\item<2-> Traditional representations do not encode semantic similarity between documents. Therefore, cannot handle synonyms
	\vfill\item<3-> Drawbacks in BOW like sparsity, high-dimensionality, inability to encode context information and consider word ordering
	\vfill\item<4-> Compositionality of word vectors beyond weighted average \cite{mitchell2010composition, zanzotto2010estimating, yessenalina2011compositional, grefenstette2013multi, mikolov2013distributed} is not simple
	\vfill\item<5-> \citet{socher2013recursive} propose a Recursive Tensor Neural Network (RTNN) to compose word vectors for learning sentence representations using the parse-tree of the sentence in a bottom-up fashion
	\begin{itemize}
		\vfill\item<6-> Parsing, a computationally expensive step required for each sentence
		\vfill\item<7-> Composing sentence vectors to represent documents is not straight-forward
	\end{itemize}
\end{enumerate}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Our Model for Learning Document Representations}
\vfill \onslide<1->{
\emph{Inspired by the log-linear models to learn word vectors, we present model, to learn universal distributed representations for documents and words } 
% \begin{quote}
% To learn universal distributed representations for documents and words
% \end{quote}
}
\vfill \onslide<2->{
\textbf{Hypothesis } 
\begin{quote}
Document Representations that encode semantic content of the document should be able to predict words in the document
\end{quote}}

\vfill \onslide<3->{
Our model, }
\begin{enumerate}
	\vfill\item<4-> Learns distributed representations for document (and words) that encode the different semantic content in the documents
	\vfill\item<5-> Embeds documents and words in the same $k$-dimensional space such that semantically similar entities have similar vector representations
\end{enumerate}

\end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Our Model for Learning Document Representations}
% \vfill\onslide<1->{
% We present an unsupervised neural network model that, }
% \begin{enumerate}
% 	\vfill\item<1-> Represents each document and word with a $k$-dimensional vector
% 	\vfill\item<2-> Estimates the probability of predicting the middle word in a given word sequence
% 	\vfill\item<3-> Learns document and word vectors and the parameters of the probability function using training corpus
% \end{enumerate}	
\vfill\onslide<1->{
We present an unsupervised neural network model that, }
\begin{enumerate}
	\vfill\item<2-> Represents each document $d_{i} \in D$ by a vector $\vecdi{i} \in \mathbb{R}^{k}$\\
	Vectors are stored as columns of the matrix $\matD = \left[\vecdi{1}, \ldots, \vecdi{|\setD|}\right] \in \mathbb{R}^{k \times |\setD|}$
	\vfill\item<3-> Each word $w_{i} \in W$, is represented by a vector $\vecwi{i} \in \mathbb{R}^{k}$
	Vectors are stored as columns of the matrix $\matW = \left[\vecwi{1}, \ldots, \vecwi{|\setW|}\right] \in \mathbb{R}^{k \times |\setW|}$
	\vfill\item<4-> Given a sequence of words, $(w_{t-c}, \ldots, w_{t+c})$ in document $d_{i}$, estimates 
	\begin{equation*}
	p(w_{t} | d_{i}, w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})
	\end{equation*}
	\vfill\item<5-> Maximizes the probability of predicting the words correctly to learn $\matD$ and $\matW$ and the parameters of the probability function
\end{enumerate}	
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Our Model for Learning Document Representations}
\vfill
\begingroup
\centering
\tikzset{every picture/.style={scale=0.4}}
\input{../figs/nn_arch}%
\endgroup
\footnotesize{
	\vfill
	\onslide<2->{
		\textbf{Context Representation} : 
		\begin{equation}
		h_{c} = \vecdi{d_{i}} + \wgt{t-c}\vecwi{w_{t-c}} + \ldots + \wgt{t-1}\vecwi{w_{t-1}} + \wgt{t+1}\vecwi{w_{t+1}} + \ldots + \wgt{t+c}\vecwi{w_{t+c}}
		\end{equation}
		\vfill
	} \onslide<3->{
		\textbf{Probability Estimation} : 
		\begin{equation}
		s_{w_{i}} = \sigma(\vecwi{w_{i}}\cdot h_{c}) \text{,} \quad  \sigma(x) = \frac{1}{1 + e^{-x}}
		\end{equation}
		\begin{equation}
		p(w_{t} | d_{i}, w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c}) = \frac{e^{s_{w_{t}}}}{\sum_{i \in \setW} e^{s_{w_{i}}}}
		\end{equation}
	}
\vfill
}
\end{frame}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Training Objective}
\vfill
\begin{enumerate}
	\vfill\item<1-> Training data $\traindata$ = $\{d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}\}^{m=M}_{m=1}$
	\vfill\item<2-> Learn optimum parameter set $\Theta = (\matD, \matW, \Lambda)$, i.e. document and word vectors and the neural network weights $\Lambda$ 
	\vfill\item<3-> Maximize average log-probability of predicting $w_{t}$ correctly in each sequence in $\traindata$
					\begin{equation}
						\hat{\Theta} =  \argmax_{\Theta}~l(\traindata, \Theta)
					\end{equation}
					\begin{equation}
						l(\traindata, \Theta) = \frac{1}{M}\sum_{m=1}^{M} \log \left[p(w^{(m)}_{t} | d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t-1}, w^{(m)}_{t+1}, \ldots, w^{(m)}_{t+c})\right]
					\end{equation}
	\vfill\item<4-> Use Stochastic Gradient Descent (SGD) to update parameters
					\begin{equation}
						\theta^{(x)}_{i} = \theta^{(x-1)}_{i} + \gamma\frac{\partial l(\traindata, \Theta)}{\partial \theta_{i}}
					\end{equation}
\end{enumerate}
	

\end{frame}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Noise Contrastive Estimation}
\vfill
\begin{enumerate}
	\vfill\item<1-> Computing soft-max for each training sequence is expensive, $\mathcal{O}$($\setW$)
	\vfill\item<2-> Speed-ups in softmax computation can be attained using Hierarchical soft-max \citep{morin2005hierarchical} and importance sampling to approximate the likelihood gradient \citep{bengio2003quick, bengio2008adaptive}
	\begin{itemize}
		\vfill\item<3-> Finding well-performing trees in Hierarchical soft-max is not trivial
		\vfill\item<4-> Importance sampling suffers from stability issues
	\end{itemize}
	\vfill\item<4-> \textbf{Noise Contrastive Estimation} (NCE) \citep{gutmann2012noise} fits unnormalized probabilities
	\begin{itemize}
		\vfill\item<5-> Reduces the problem of \emph{probability density estimation} to \emph{probabilistic binary classification}
		\vfill\item<6-> Adaptation to NPLM \citep{mnih2012fast} and learning word embeddings \citep{mnih2013learning} show significant training time speed-ups
	\end{itemize}
\end{enumerate}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{References}
    %{\footnotesize
    \bibliographystyle{abbrvnat}
    \bibliography{references}
    %}
\end{frame}


\end{document}