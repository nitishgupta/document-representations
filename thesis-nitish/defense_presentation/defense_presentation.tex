\documentclass[10pt]{beamer}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{color}
\usepackage[numbers]{natbib}
\usepackage{framed}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{chngpage}
\usepackage{caption}
\usepackage{tikz}
\usepackage{mdwlist}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}



\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO:} #1}}
\newcommand{\setD}{\ensuremath{D} }
\newcommand{\setW}{\ensuremath{V} }
\newcommand{\setC}{\ensuremath{C} }
\newcommand{\matD}{\ensuremath{\mathrm{D}} }
\newcommand{\matW}{\ensuremath{\mathrm{W}} }
\newcommand{\matC}{\ensuremath{\mathrm{C}} }
\newcommand{\vecdi}[1]{\ensuremath{\mathrm{v}^{D}_{#1}}}
\newcommand{\vecwi}[1]{\ensuremath{\mathrm{v}^{W}_{#1}}}
\newcommand{\vecci}[1]{\ensuremath{\mathrm{v}^{C}_{#1}}}
\newcommand{\con}{$(w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})$}
\newcommand{\wgt}[1]{\ensuremath{\lambda_{#1}}}
\newcommand{\traindata}{\ensuremath{\mathcal{T}}}
\newcommand{\db}{\ensuremath{\mathcal{D}} }
\newcommand{\para}[1]{\noindent\textbf{%\fontsize{14}{15}\selectfont 
#1}} %{\paragraph{#1}}
\newcommand{\highest}[1]{\textbf{#1}}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\usetheme{Madrid}
%\usetheme{umbc1}
\title[{Distributed Document Representations for Multi-Label Document Categorization}]{Learning Distributed Document Representations for Multi-Label Document Categorization}
\author{\textbf{Nitish Gupta}}
\date{May 16, 2015}
\institute[IITK]{B.Tech - M.Tech Dual Degree\\
\vspace{.2cm}Thesis Defense\\ 
\vspace{.2cm}Electrical Engineering\\
\vspace{.2cm}IIT Kanpur }
\begin{document}
\setbeamercovered{clear}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}
\titlepage
%\begin{figure}[ht]
%\begin{center}
%\includegraphics[width=1.75cm]{mips}
%\epsfig{file=iitk_logo.eps,width=1.5cm}
%\end{center}
%\end{figure}
\begin{figure}[ht]
\begin{center}
\includegraphics[width=1.75cm]{iitk_logo-eps-converted-to.pdf}
%\epsfig{file=iitk_logo.eps,width=1.5in}
\end{center}
\end{figure}
%\begin{figure}
%\begin{center}
%\hspace{-.0cm}
%\vspace{2cm}\includegraphics[width=1.25cm]{mips-eps-converted-to.pdf}
%\end{center}
%\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Outline}
\begin{enumerate}
	\vfill\item Multi-Label Document Categorization
	\vfill\item Related Work
	\begin{itemize}
	  \vfill\item Text Representations
	  \vfill\item Learning Algorithms
	\end{itemize}  
	\vfill\item Distributed Word Representations
	\vfill\item Learning Distributed Document Representations
	\vfill\item Document Categorization Algorithm
	\vfill\item Results
	\vfill\item Conclusion and Future Work
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Introduction to Multi-Label Document Categorization}
\begin{itemize}
	\vfill\item<1-> Text Documents usually belong to more than one conceptual class. \\For E.g. an article on Music Piracy 
	\vfill\item<2-> Task of assigning documents to one or more predefined categories is called \emph{Multi-Label Document Categorization}
	\vfill\item<3-> Wide range real-world applications :
	\begin{itemize} 
	  \vfill\item<3-> Web-page tagging
	  \vfill\item<3-> Medical Patient Record Management
	  \vfill\item<3-> Wikipedia Article Management
	  \vfill\item<3-> Document Recommendation etc.
	\end{itemize} 
	\vfill\item<4-> Multi-label classification belongs to a general class of supervised learning algorithms where : 
	\begin{itemize}
	  \vfill\item<5-> Training instances in the form of document-category pairs are used to learn a classifier $\mathcal{H}$ 
	  \vfill\item<6-> Learned classifier $\mathcal{H}$ is used to assign categories to new test documents
	\end{itemize}
\end{itemize}
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Introduction to Multi-Label Document Categorization}
%\onslide<1->{ \textbf{Problem Statement}, } \\
\vfill\onslide<1->{ Given, }
\begin{itemize} 
	\vfill\item<1-> A set of documents $D = \{d_{1}, \ldots, d_{|D|}\}$
	\vfill\item<2-> A set of categories $C = \{c_{1}, \ldots, c_{|C|}\}$
	\vfill\item<3-> Training data for $n$ {\small ($n < |D|$)} documents, $\traindata = \{l_{d_{1}}, \ldots, l_{d_{n}}\}$ 
\onslide<4->{   \\ Each label vector $l_{d_{i}} \in \{0,1\}^{|C|}$ denotes relevance of categories to the document $d_{i}$ }
\end{itemize} 
\vfill
\onslide<5->{
	Example :
	\begin{table}[h!]
	\tabcolsep=0.1cm
	\tiny
	\begin{center}
	\begin{tabular}{c@{\hskip5mm} c c c c c c}
	\toprule
	\textbf{Documents}	&	\textbf{Sports} & \textbf{Music} & \textbf{Arts} & \textbf{Technology}  & \textbf{Literature} & \textbf{Politics}\\
	\cmidrule{1-1}
	\cmidrule{2-7}
	$d_{1}$ & 0 & 0 & 1 & 0 & 1 & 0\\
	$d_{2}$ & 0 & 1 & 1 & 0 & 0 & 1\\
	$d_{3}$ & 1 & 0 & 0 & 1 & 0 & 1\\
	$d_{4}$ & x & x & x & x & x & x\\
	$d_{5}$ & x & x & x & x & x & x\\
	\bottomrule         
	\end{tabular}
	\end{center}
	\end{table}
}
\vfill
\onslide<6->{
Using $\traindata$, $D$ and $C$ the learning algorithm learns a multi-label classifier $\mathcal{H}$ to estimate category label vectors, $l_{d_{j}}$ {\small $(j>n)$} for the test documents. 
}
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Introduction to Multi-Label Document Categorization}
\vfill\onslide<1->{ Document Categorization task has the following two components : }
\begin{enumerate}
	\vfill\item<2-> \emph{Learning Document Representations} : Representing text documents using numerical vectors that are inputs to the multi-label classifier $\mathcal{H}$
	\begin{itemize}
	 	\vfill\item<3-> Each document $d_{i} \in D$ is represented using a vector $v_{d_{i}} \in \mathbb{R}^{k}$ \\
	 	\vfill\item<4->	Vectors ($v_{d_{i}}$) should encode the semantic content of the documents 
	 	\vfill\item<5-> Encoding documents in a $k$-dimensional space using such representation is called the \emph{Vector Space Model}
	 	\vfill\item<6-> The complete document set $D$ can be represented by a document representation matrix $\matD \in \mathbb{R}^{k \times |D|}$
	\end{itemize}
\suspend{enumerate} \vfill
\onslide<7->{	
\emph{ In this thesis, we focus on learning efficient document representations, $\matD$}	}
\vfill
	\resume{enumerate}
	\vfill\item<8-> \emph{Learning Algorithm} : Algorithm to learn the multi-label classifier $\mathcal{H}$
\end{enumerate}\vfill 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Background on Learning Algorithms}
\vfill
%\onslide<1->{ Learning algorithms can be classified into two classes :  }
\begin{enumerate}
	\vfill\item<2-> \emph{Learning Multiple Binary Classifiers} : \\
	\onslide<3->{ {\footnotesize	Algorithms that treat each category assignment independently and learn multiple binary classifiers, one for each category, to make the category assignments} 	} 
	\begin{itemize}
		\vfill\item<4-> Logistic Regression
		\vfill\item<5-> Support Vector Machines (SVM)
		\vfill\item<6-> Neural Networks
		\vfill\item<7-> Naive Bayes
	\end{itemize}

	\vfill\item<8-> \emph{Learning Single Joint Classifier} : \\
	\onslide<9->{ {\footnotesize	Algorithms that jointly assign all the categories to a document $d_{i}$, i.e. estimate the complete label vector $l_{d_{i}}$ using a single classifier} 	}
	\begin{itemize}
		\vfill\item<10-> k-Nearest Neighbor (k-NN)
		\vfill\item<11-> Linear Least Square Fit
		\vfill\item<12-> Decision Trees
		\vfill\item<13-> Generative Probabilistic Models
	\end{itemize}
\end{enumerate}
%\onslide<14->{ Learning a single joint classifier is usually better as it is able to exploit the category correlations }
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Background on Text Representation}
\vfill
\onslide<1->{
\textbf{Bag of Words Model} }
%Most common method to learn vector representations for a set of documents is the \emph{Bag of Words (BOW)} model 	}
\begin{itemize}
	\vfill\item<2-> Document $d_{i}$ represented by $v_{d_{i}} \in \mathbb{R}^{|V|}$
	\vfill\item<3-> Each element in $v_{d_{i}}$ denotes presence/absence of each word
	\vfill\item<4-> Weighing techniques employed to give importance to important terms
	\begin{itemize}
		\vfill\item<5-> Term Frequency (\emph{tf} )
		\vfill\item<5-> Inverse Document Frequency (\emph{idf} )
		\vfill\item<5-> Term Frequency - Inverse Document Frequency (\emph{tf-idf} ) : \emph{tf} $\times$\emph{idf}
	\end{itemize}
\end{itemize}
\vfill
\onslide<6->{
\textbf{Drawbacks of the Bag-of-Words model} }
\begin{itemize}
	\vfill\item<7-> High-dimensionality
	\vfill\item<8-> Sparsity
	\vfill\item<9-> Inability to encode word contexts
	\vfill\item<10-> Ignores word order
\end{itemize}
\vfill
% \todo{feature selection. Word Embeddings. Our document embedding. Our docu cate. Datasets. Results End}
% \vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Background on Feature Selection / Dimensionality Reduction}
\vfill
\onslide<1-> {
Techniques to deal with sparsity and high-dimensionality in BOW }
\begin{itemize}
	\vfill\item<2-> Information Gain \\
	\onslide<2->{ { \scriptsize
	\begin{equation}
	G(t) = -\sum_{i=1}^{|C|} P(c_{i})\log P(c_{i}) + P(t)\sum_{i=1}^{|C|} P(c_{i}|t)\log P(c_{i}|t) + P(\sim t)\sum_{i=1}^{|C|} P(c_{i}|\sim t)\log P(c_{i}|\sim t)
	\end{equation}
	} }
	\vfill\item<3-> Mutual Information \\
	\onslide<3->{ { \scriptsize
	\begin{equation}
	I(t,c) = \log \frac{P(t \wedge c)}{P(t) \times P(c)} \text{,}\qquad I_{avg}(t) = \sum_{i=1}^{|C|} P(c_{i})I(t,c_{i})
	\end{equation}
	} }
	\vfill\item<4-> Latent Semantic Indexing (LSI) \\
	\onslide<4->{ { \scriptsize
	\begin{equation}
	X = T S D^{T}
	\end{equation}
	} }
\end{itemize}
\vfill
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Distributed Word Representations}
%\vfill
\onslide<1->{
Representation of each word $w_{i}$ using vector $v_{w_{i}} \in \mathbb{R}^{k}$ ($k \in [50, 300]$) } \\ \vspace{0.5cm}
\onslide<2->{
\textbf{ Need for Distributed Word Representations} } 
\begin{itemize}
	\vfill\item<3-> Curse of Dimensionality
	\begin{itemize}
		\vfill\item<4-> One-hot representations grow with the size of vocabulary
		\vfill\item<5-> Parameters in language modeling grow exponentially with the size of vocabulary
	\end{itemize}
	\vfill\item<6-> No Word Similarity Measure
	\begin{itemize}
		\vfill\item<7-> One-hot representations are orthogonal representations
		\vfill\item<8-> Cannot capture semantic similarity between words
	\end{itemize}
\end{itemize}
\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Neural Probabilistic Language Model}
\vfill
\onslide<1->{
\citet{bengio2003neural} developed \emph{Neural Probabilistic Language Model (NPLM)} to learn}%learn distributed word vectors and a probability function that uses these vectors to learn a statistical model of language	}
\begin{enumerate}
\vfill\item<2-> Distributed word vectors
\vfill\item<3-> A probability function that, using these word vectors, learns a statistical model of language 
\end{enumerate}

\begin{columns}[T]
 \begin{column}{.5\textwidth}
 	\centering
 	\onslide<4->{
 	\includegraphics[scale=0.15]{../figs/bengio_nn.png} }

 \end{column}

 \begin{column}{.5\textwidth}
 	\centering
 	{\footnotesize
 	\vfill \onslide<5->{
 	\begin{equation}
    P(w_{t} | w_{1}^{t-1}) \approx P(w_{t} | w_{t-n+1}^{t-1})
    \end{equation} }
    \vfill \onslide<6->{
    \begin{equation}
	y = b + U tanh(d + Hx) \text{,} \quad y \in \mathbb{R}^{|V|}
	\end{equation} }
	\vfill \onslide<7->{
	\begin{equation}
	P(w_{t} = i | w_{t-1}, \ldots, w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}e^{y_{i}}}
	\end{equation} }
	}
 \end{column}
\end{columns}

\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Log-Linear Models}
\vfill
\onslide<1->{
\emph{Log-Linear Models} for learning distributed word vectors are proposed in \citet{mikolov2013efficient}. These models use word vectors to predict other words in the context.	 }
% \begin{itemize}
% \vfill\item<2-> do not build a language model hence also consider future words in contexts
% \vfill\item<3-> show that the word vectors capture the semantic similarity between words
% \end{itemize}
\vfill
\begin{enumerate}
	\vfill\item<2-> Continuous Bag-of-Words Model 
	\begin{columns}[T]

	 \begin{column}{.5\textwidth}
	 	\centering
	 	\onslide<2->{
	 	\includegraphics[scale=0.12]{../figs/mikolov_cbow.png} }
	 \end{column}
	 \begin{column}{.5\textwidth}
	 	\centering
	 	{\footnotesize
	 	\onslide<3->{
	 	\begin{equation}
	    h = w_{t-k} + \ldots + w_{t-1} + w_{t+1} + \dots + w_{t+k}
	    \end{equation} }
	    \onslide<4->{
	    \begin{equation}
		y = b + Uh \text{,} \quad y \in \mathbb{R}^{|V|}
		\end{equation} }
		\onslide<5->{
		\begin{equation}
		P(w_{t}|w_{t-k}, \ldots, w_{t+k}) = \frac{e^{y_{w_t}}}{\sum_{i} e^{y_{i}}}
		\end{equation} }
		}
	 \end{column}
	\end{columns}

	\vfill\item<6-> Skip-Gram Model
	\begin{columns}[T]

	 \begin{column}{.5\textwidth}
	 	\centering
	 	\onslide<6->{
	 	\includegraphics[scale=0.12]{../figs/mikolov_skip.png} }
	 \end{column}
	 \begin{column}{.5\textwidth}
	 	\centering
	 	\vfill
	 	{\footnotesize
	 	\onslide<7->{
	 	\begin{equation}
	    P(w_{t+j}|w_{t}) = \frac{e^{(v_{w_{t}} \cdot v_{w_{t+j}} )} }{\sum_{i} e^{(v_{w_{t}} \cdot v_{w_{i}})} }
	    \end{equation} }
	    }
	 \end{column}
	\end{columns}
\end{enumerate}

\vfill
\end{frame}

%%%%%%%%%%%%%%%%%%%

\begin{frame}{Distributed Document Representations}
\vfill \onslide<1->{
\textbf{Motivation for learning distributed document representations} }
\begin{enumerate}
	\vfill\item<2-> Traditional representations do not encode semantic similarity between documents. Therefore, cannot handle synonyms
	\vfill\item<3-> Drawbacks in BOW like sparsity, high-dimensionality, inability to encode context information and consider word ordering
	\vfill\item<4-> Compositionality of word vectors beyond weighted average \cite{mitchell2010composition, zanzotto2010estimating, yessenalina2011compositional, grefenstette2013multi, mikolov2013distributed} is not simple
	\vfill\item<5-> \citet{socher2013recursive} propose a Recursive Tensor Neural Network (RTNN) to compose word vectors for learning sentence representations using the parse-tree of the sentence in a bottom-up fashion
	\begin{itemize}
		\vfill\item<6-> Parsing, a computationally expensive step required for each sentence
		\vfill\item<7-> Composing sentence vectors to represent documents is not straight-forward
	\end{itemize}
\end{enumerate}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Our Model for Learning Document Representations}
\vfill \onslide<1->{
\emph{Inspired by the log-linear models to learn word vectors, we present model, to learn universal distributed representations for documents and words } 
% \begin{quote}
% To learn universal distributed representations for documents and words
% \end{quote}
}
\vfill \onslide<2->{
\textbf{Hypothesis } 
\begin{quote}
Document Representations that encode semantic content of the document should be able to predict words in the document
\end{quote}}

\vfill \onslide<3->{
Our model, }
\begin{enumerate}
	\vfill\item<4-> Learns distributed representations for document (and words) that encode the different semantic content in the documents
	\vfill\item<5-> Embeds documents and words in the same $k$-dimensional space such that semantically similar entities have similar vector representations
\end{enumerate}

\end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Our Model for Learning Document Representations}
% \vfill\onslide<1->{
% We present an unsupervised neural network model that, }
% \begin{enumerate}
% 	\vfill\item<1-> Represents each document and word with a $k$-dimensional vector
% 	\vfill\item<2-> Estimates the probability of predicting the middle word in a given word sequence
% 	\vfill\item<3-> Learns document and word vectors and the parameters of the probability function using training corpus
% \end{enumerate}	
\vfill\onslide<1->{
We present an unsupervised neural network model that, }
\begin{enumerate}
	\vfill\item<2-> Represents each document $d_{i} \in D$ by a vector $\vecdi{i} \in \mathbb{R}^{k}$\\
	Vectors are stored as columns of the matrix $\matD = \left[\vecdi{1}, \ldots, \vecdi{|\setD|}\right] \in \mathbb{R}^{k \times |\setD|}$
	\vfill\item<3-> Each word $w_{i} \in W$, is represented by a vector $\vecwi{i} \in \mathbb{R}^{k}$
	Vectors are stored as columns of the matrix $\matW = \left[\vecwi{1}, \ldots, \vecwi{|\setW|}\right] \in \mathbb{R}^{k \times |\setW|}$
	\vfill\item<4-> Given a sequence of words, $(w_{t-c}, \ldots, w_{t+c})$ in document $d_{i}$, estimates 
	\begin{equation*}
	p(w_{t} | d_{i}, w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})
	\end{equation*}
	\vfill\item<5-> Maximizes the probability of predicting the words correctly to learn $\matD$ and $\matW$ and the parameters of the probability function
\end{enumerate}	
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Our Model for Learning Document Representations}
\vfill
\begingroup
\centering
\tikzset{every picture/.style={scale=0.4}}
\input{../figs/nn_arch}%
\endgroup
\footnotesize{
	\vfill
	\onslide<2->{
		\textbf{Context Representation} : 
		\begin{equation}
		h_{c} = \vecdi{d_{i}} + \wgt{t-c}\vecwi{w_{t-c}} + \ldots + \wgt{t-1}\vecwi{w_{t-1}} + \wgt{t+1}\vecwi{w_{t+1}} + \ldots + \wgt{t+c}\vecwi{w_{t+c}}
		\end{equation}
		\vfill
	} \onslide<3->{
		\textbf{Probability Estimation} : 
		\begin{equation}
		s_{w_{i}} = \sigma(\vecwi{w_{i}}\cdot h_{c}) \text{,} \quad  \sigma(x) = \frac{1}{1 + e^{-x}}
		\end{equation}
		\begin{equation}
		p(w_{t} | d_{i}, w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c}) = \frac{e^{s_{w_{t}}}}{\sum_{i \in \setW} e^{s_{w_{i}}}}
		\end{equation}
	}
\vfill
}
\end{frame}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Training Objective}
\vfill
\begin{enumerate}
	\vfill\item<1-> Training data $\traindata$ = $\{d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}\}^{m=M}_{m=1}$
	\vfill\item<2-> Learn optimum parameter set $\Theta = (\matD, \matW, \Lambda)$, i.e. document and word vectors and the neural network weights $\Lambda$ 
	\vfill\item<3-> Maximize average log-probability of predicting $w_{t}$ correctly in each sequence in $\traindata$
					\begin{equation}
						\hat{\Theta} =  \argmax_{\Theta}~l(\traindata, \Theta)
					\end{equation}
					\begin{equation}
						l(\traindata, \Theta) = \frac{1}{M}\sum_{m=1}^{M} \log \left[p(w^{(m)}_{t} | d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t-1}, w^{(m)}_{t+1}, \ldots, w^{(m)}_{t+c})\right]
					\end{equation}
	\vfill\item<4-> Use Stochastic Gradient Descent (SGD) to update parameters
					\begin{equation}
						\theta^{(x)}_{i} = \theta^{(x-1)}_{i} + \gamma\frac{\partial l(\traindata, \Theta)}{\partial \theta_{i}}
					\end{equation}
\end{enumerate}
	

\end{frame}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Noise Contrastive Estimation}
\vfill
\begin{enumerate}
	\vfill\item<1-> Soft-max computation is expensive, $\mathcal{O}$($\setW$)
	\vfill\item<2-> Speed-ups using \textbf{Hierarchical soft-max} \citep{morin2005hierarchical} and \textbf{Importance sampling} to approximate the likelihood gradient \citep{bengio2003quick, bengio2008adaptive}
	\begin{itemize}
		\vfill\item<3-> Finding well-performing trees in Hierarchical soft-max is not trivial
		\vfill\item<4-> Importance sampling suffers from stability issues
	\end{itemize}
	\vfill\item<5-> \textbf{Noise Contrastive Estimation} (NCE) \citep{gutmann2012noise} fits unnormalized probabilities
	\begin{itemize}
		\vfill\item<6-> Reduces the problem of \emph{probability density estimation} to \emph{probabilistic binary classification}
		\vfill\item<7-> Adaptation to NPLM \citep{mnih2012fast} and learning word embeddings \citep{mnih2013learning} show significant training time speed-ups
	\end{itemize}
\end{enumerate}
\end{frame}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Noise Contrastive Estimation (contd.)}
\vfill
\begin{enumerate}
	\vfill\item<1-> Given a sequence of words $(w_{t-c}, \ldots, w_{t+c})$ in document $d_{i}$
	\begin{itemize}
		\vfill\item<2-> \emph{Earlier objective} : Maximize $p(w_{t} | d_{i}, w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})$	
		\vfill\item<3-> \emph{New objective}	 : Build binary classifier to distinguish between correct middle word $w_{t}$ and random corrupt word 	
	\end{itemize}
\end{enumerate}
\vfill \onslide<4->{
\textbf{For NCE Binary Classification Objective : }
}
\begin{enumerate}
	\vfill\item<5-> New labeled training data : $\traindata$ = $\{d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}, Y^{(m)}=1\}^{m=M}_{m=1}$
	\vfill\item<6-> For every positive training sequence, $n$ negative training sequences introduced where,
	\begin{itemize}
		\vfill\item<7-> The observed middle word $w_{t}$ is replaced by a corrupt word $w_{x}$ drawn from a noise distribution $P_{n}(w)$
		\vfill\item<8-> The label for the negative sequence is $Y = 0$
	\end{itemize}
	\vfill\item<9-> Complete training data : $\traindata$ = $\{d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}, Y^{(m)}\}^{m=M+nM}_{m=1}$
\end{enumerate}

\end{frame}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Noise Contrastive Estimation (contd.)}
\vfill
\vfill\onslide<1-> {
 	Given a sequence of words $(w_{t-c}, \ldots, w_{t+c})$ in document $d_{i}$, our new objective is to predict whether the sequence is legitimate
}
\begin{itemize}
	\vfill\item<2-> We build a probabilistic binary classifier to predict the label $Y$
	\vfill\onslide<3-> { \begin{equation}
	P(Y=1|d_{i}, w_{t-c}, \ldots, w_{t+c}, \Theta) = \sigma(\vecwi{w_{t}} \cdot h_{c})
	\end{equation} }
	\vfill\onslide<4-> { \begin{equation}
	P(Y=0|d_{i}, w_{t-c}, \ldots, w_{t+c}, \Theta) = 1 - \sigma(\vecwi{w_{t}} \cdot h_{c})
	\end{equation} }
	\vfill\onslide<5-> { \begin{equation}
	P(Y|d_{i}, w_{t-c}, \ldots, w_{t+c}, \Theta) = [\sigma(\vecwi{w_{t}} \cdot h_{c})]^{Y}[1 - \sigma(\vecwi{w_{t}} \cdot h_{c})]^{1-Y}
	\end{equation} }
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Learning Objective with NCE}
\vfill
\vfill\onslide<1-> {
 	Given the training data $\traindata$ = $\{d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}, Y^{(m)}\}^{m=M+nM}_{m=1}$, we maximize the log-likelihood of observing it
}
\vfill\onslide<2-> { \begin{equation}
 	\hat{\Theta} =  \argmax_{\Theta}~l(\traindata, \Theta)
 	\end{equation}
 	\begin{equation}
 	l(\traindata, \Theta) = \sum_{m=1}^{M + nM} \log P_{\Theta}(Y_{m} = Y^{(m)})
 	\end{equation}
 	\blfootnote{$P_{\Theta}(Y_{m})$ is a shorthand notation for $P(Y_{m}|d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}, \Theta)$ }
 	\blfootnote{$Y_{m}$ is the predicted label}
}
\vfill\onslide<3-> {
The logarithm of the probability estimate is given by,
\footnotesize{
\begin{equation}
\log P_{\Theta}(Y_{m} = Y^{(m)}) = Y^{(m)}\log \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c}) + (1 - Y^{(m)})\log (1 - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c}))
\end{equation} } }

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Parameter Estimation}

\vfill\onslide<1-> {
 	We use SGD to learn parameters i.e. document and word vectors and the neural network weights
 	\begin{equation}
		\theta^{(x)}_{i} = \theta^{(x-1)}_{i} + \gamma\frac{\partial l(\traindata, \Theta)}{\partial \theta_{i}}
	\end{equation}
}
\vfill\onslide<2-> { Gradient of $\log P_{\Theta}(Y_{m} = Y^{(m)})$ with respect to parameter $\theta$, }
\vfill\onslide<3-> { \scriptsize{	\begin{equation}
		\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \theta} = \left[ Y^{(m)}\frac{1}{\sigma(d^{(m)})} - (1-Y^{(m)})\frac{1}{(1 - \sigma(d^{(m)}))}\right] \frac{\partial \sigma(d^{(m)})}{\partial \theta} 
\end{equation} } }
\vfill\onslide<4-> { \scriptsize{	\begin{equation}
		\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \theta} = \left[ Y^{(m)}\frac{1}{ \sigma(d^{(m)})} - (1 - Y^{(m)})\frac{1}{(1 - \sigma(d^{(m)}))}\right]
		\left[\sigma(d^{(m)})(1-\sigma(d^{(m)}))\right]\frac{\partial d^{(m)}}{\partial \theta}
\end{equation} } }
\vfill\onslide<5-> { \scriptsize{	\begin{equation}
		\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \theta} = \left[ Y^{(m)} - \sigma(d^{(m)})\right] \frac{\partial d^{(m)}}{\partial \theta}
\end{equation} } }
\vfill\onslide<6-> { \scriptsize{	\begin{equation}
		\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \theta} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] \frac{\partial (\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})}{\partial \theta}
\end{equation} } }

\blfootnote{\tiny{$d= \mathrm{v}^{W}_{w_{t}} \cdot h_{c}$, is the pre-sigmoid activation } }

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}{Parameter Estimation}

% \vfill\onslide<1-> { Gradient of the likelihood w.r.t. various parameters, }

% \begin{enumerate}
% 	\vfill\item<2-> Document Vector : 
% 		\scriptsize{ \begin{equation}
% 				\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \vecdi{d^{(m)}_{i}}} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] \vecwi{w^{(m)}_{t}}
% 		\end{equation} }
% 	\vfill\item<3-> Middle Word Vector : 	
% 		\scriptsize{ \begin{equation}
% 			\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \vecwi{w^{(m)}_{t}}} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] h^{(m)}_{c}
% 		\end{equation} }
% 	\vfill\item<4-> Context Word Vectors : 		
% 		\scriptsize{	\begin{equation}
% 			\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \vecwi{w^{(m)}_{t+j}}} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] \wgt{t+j} \vecwi{w^{(m)}_{t}}
% 		\end{equation} }
% 	\vfill\item<5-> Neural Network Weights : 		
% 		\scriptsize{	\begin{equation}
% 			\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \wgt{t+j}} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] (\vecwi{w^{(m)}_{t}} \cdot \vecwi{w^{(m)}_{t+j}})
% 		\end{equation} }
% \end{enumerate}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Update rule for Parameters}

\begin{enumerate}
	\vfill\item<1-> Document Vector : 
		\scriptsize{ \begin{equation}
				(\vecdi{d^{(m)}_{i}})^{(i+1)} = (\vecdi{d^{(m)}_{i}})^{(i)} + \gamma \left[ (Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c}) )\vecwi{w^{(m)}_{t}} - \beta\vecdi{d^{(m)}_{i}} \right]
		\end{equation} }
	\vfill\item<2-> Middle Word Vector : 	
		\scriptsize{ \begin{equation}
			(\vecwi{w^{(m)}_{t}})^{(i+1)} = (\vecwi{w^{(m)}_{t}})^{(i)} + \gamma \left[ (Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})) h^{(m)}_{c} - \beta\vecwi{w^{(m)}_{t}} \right]
		\end{equation} }
	\vfill\item<3-> Context Word Vectors : 		
		\scriptsize{	\begin{equation}
			(\vecwi{w^{(m)}_{t+j}})^{(i+1)} = (\vecwi{w^{(m)}_{t+j}})^{(i)} + \gamma \left[(Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c}))\wgt{t+j} \vecwi{w^{(m)}_{t}} - \beta\vecwi{w^{(m)}_{t+j}} \right]
		\end{equation} }
	\vfill\item<4-> Neural Network Weights : 		
		\scriptsize{	\begin{equation}
			\wgt{t+j}^{(i+1)} = \wgt{t+j}^{(i)} + \gamma \left[ (Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c}))(\vecwi{w^{(m)}_{t}} \cdot \vecwi{w^{(m)}_{t+j}}) -\beta\wgt{t+j} \right]
		\end{equation} }
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Algorithm for learning Document Representations}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\footnotesize{
 \onslide<1->{
 \State \textbf{Input: } $\setD$, $k$, $c$, $n$, $\beta$, $\gamma$, $epochs$
 \State \textbf{Output: } Document Vectors $\matD$, Word Vectors $\matW$ 
 } \onslide<2->{
 \State $\setW \gets Extractfrom(\setD)$ 
 \State $\matD \gets random(\mathbb{R}^{k \times |\setD|})$
 \State $\matW \gets random(\mathbb{R}^{k \times |\setW|})$ 
 \State $\traindata \gets Extract from (\setD, c, n)$ \Comment{$|\traindata| = M + nM$}
 \State $\Lambda \gets \mathbf{1}^{2c}$ \Comment{$2c$-sized vector of 1s} 
 } \onslide<3->{
 \While{$epochs \geq 1$}
 \ForAll{$\{d_{i}, w_{t-c}, \ldots, w_{t+c}, Y\} \in \traindata$}
  \State $h_{c} \gets \vecdi{d_{i}} + \wgt{t-c}\vecwi{w_{t-c}} + \ldots + \wgt{t+c}\vecwi{w_{t+c}}$ 
  } \onslide<4->{
  \State $\vecdi{d_{i}} \gets \vecdi{d_{i}} + \gamma \left[ (Y - \sigma(\vecwi{w_{t}} \cdot h_{c}))\vecwi{w_{t}} - \beta\vecdi{d_{i}} \right]$
  } \onslide<5->{
  \State $\vecwi{w_{t}} \gets \vecwi{w_{t}} + \gamma \left[ (Y - \sigma(\vecwi{w_{t}} \cdot h_{c})) h_{c} - \beta\vecwi{w_{t}} \right]$
  } \onslide<6->{
  \ForAll{ $j$ $\in$ $\{t-c, \ldots, t-1, t+1, \ldots, t+c \}$} 
	\State $\vecwi{w_{t+j}} \gets \vecwi{w_{t+j}} + \gamma \left[(Y - \sigma(\vecwi{w_{t}} \cdot h_{c}))\wgt{t+j} \vecwi{w_{t}} - \beta\vecwi{w_{t+j}} \right]$
	} \onslide<7->{
  	\State $\wgt{t+j} \gets \wgt{t+j} + \gamma \left[ (Y - \sigma(\vecwi{w_{t}} \cdot h_{c}))(\vecwi{w_{t}} \cdot \vecwi{w_{t+j}}) -\beta\wgt{t+j} \right]$
  \EndFor 
  } \onslide<8->{
  \State $epochs \gets epochs - 1$
  %compute(\vecdi{d_{i}}, \vecwi{w_{t-c}}, \ldots, \vecwi{w_{t-1}}, \vecwi{w_{t+1}}, \vecwi{w_{t+c}})$
 \EndFor 
 \EndWhile
 \\
  \Return $\matD, \matW$
 }
 }
\end{algorithmic}
\end{algorithm}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Hyper-parameters of the Model}

\begin{enumerate}
	\vfill\item<1-> Embedding Dimensionality ($k$)
	\vfill\item<2-> Window  Size ($c$)
	\vfill\item<3-> Number of Negative Samples ($n$)
	\vfill\item<4-> Number of Epochs ($epochs$)
	\vfill\item<5-> Learning Rate ($\gamma$)
	\vfill\item<6-> Regularization Constant ($\beta$)
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Document Categorization using Logistic Regression}
Given,
\begin{enumerate}
	\vfill\item<1-> Set of documents, $\setD = \{d_{1}, \ldots, d_{|\setD|}\}$
	\vfill\item<2-> Set of categories, $\setC = \{c_{1}, \ldots, c_{|\setC|}\}$
	\vfill\item<3-> Training Data, $\traindata = \{ d^{(m)}_{i}, c^{(m)}_{j}, y^{(m)}\}^{m=T}_{m=1}$, $y^{(m)} \in \{0,1\}$
\end{enumerate}

\vfill\onslide<4->{ 
The task is to assign categories to a new document $d_{x}$ \\To model document category relation, we }

\begin{enumerate}
	\vfill\item<5-> Represent each $c_{i} \in \setC$ using $\vecci{c_{i}} \in \mathbb{R}^{k}$
	\vfill\item<6-> Learn a probabilistic logistic classifier to assign categories
	% \vfill\item<7-> Probability of  $c_{j}$ relevant to $d_{i}$ is estimated using
	% \begin{equation}
	% P(y=1|d_{i}, c_{j}, \matD, \matC) = \sigma(\vecdi{d_{i}}\cdot \vecci{c_{j}})
	% \end{equation}
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Logistic Classifier for Categorization}
\vfill
\vfill\onslide<1-> {
 	Given document category pair, $\{d_{i}, c_{j}\}$,
}
\begin{itemize}
	\vfill\item<2-> We build a probabilistic logistic classifier to predict the label $y$
	\vfill\onslide<3-> { \begin{equation}
	P(y=1|d_{i}, c_{j}, \matD, \matC) = \sigma(\vecdi{d_{i}}\cdot \vecci{c_{j}})
	\end{equation} }
	\vfill\onslide<4-> { \begin{equation}
	P(y=0|d_{i}, c_{j}, \matD, \matC) = 1 - \sigma(\vecdi{d_{i}}\cdot \vecci{c_{j}})
	\end{equation} }
	\vfill\onslide<5-> { \begin{equation}
	P(y|d_{i}, c_{j}, \matD, \matC) = \sigma(\vecdi{d_{i}}\cdot \vecci{c_{j}})^{y}(1 - \sigma(\vecdi{d_{i}}\cdot \vecci{c_{j}}))^{1-y}
	\end{equation} }
	\vfill\onslide<6-> { \begin{equation}
	\log P(y|d_{i}, c_{j}, \matD, \matC) = y\log \sigma(\vecdi{d_{i}}\cdot \vecci{c_{j}}) + (1 - y)\log (1 - \sigma(\vecdi{d_{i}}\cdot \vecci{c_{j}}))
	\end{equation} }
\end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Learning Category Embeddings}
\vfill
\vfill\onslide<1-> {
 	Given the training data $\traindata = \{ d^{(m)}_{i}, c^{(m)}_{j}, y^{(m)}\}^{m=T}_{m=1}$, learn category embeddings ($\Theta = \setC$) by maximizing log-likelihood of training data
}
\vfill\onslide<2-> { \begin{equation}
 	\hat{\Theta} =  \argmax_{\Theta}~l(\traindata, \Theta)
 	\end{equation}
 	\begin{equation}
 	l(\traindata, \Theta) = \sum_{m=1}^{T} \log P_{\matD, \matC}(y_{m} = y^{(m)})
 	\end{equation}
 	\blfootnote{$P_{\matD, \matC}(y_{m} = y^{(m)})$ is a shorthand notation for $P(y_{m} = y^{(m)}|d_{i}, c_{j}, \matD, \matC)$ }
 	\blfootnote{$y_{m}$ is the predicted label}
}
\vfill\onslide<3-> {
Similar to learning document embeddings, category embeddings updates are given by,
\footnotesize{
\begin{equation}
(\vecci{c^{(m)}_{j}})^{(i+1)} = (\vecci{c^{(m)}_{j}})^{(i)} + \gamma \left[ ( y^{(m)} - \sigma(\vecdi{d^{(m)}_{i}}\cdot \vecci{c^{(m)}_{j}})) \vecdi{d^{(m)}_{i}} -\beta\vecci{c^{(m)}_{j}} \right]
\end{equation} } }

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Algorithm for learning Document Representations}
\begin{algorithm}[H]
\begin{algorithmic}[1]
 \State \textbf{Input: } $\matD$, $\setC$, $\traindata$, $k$, $\beta$, $\gamma$
 \State \textbf{Output: } Category Vectors $\matC$
 \State $\matC \gets random(\mathbb{R}^{k \times |\setC|})$
 \While{not converged}
 \ForAll{$\{ d_{i}, c_{j}, y\} \in \traindata$}
  \State $\vecci{c_{j}} \gets \vecci{c_{j}} + \gamma \left[ ( y - \sigma(\vecdi{d_{i}}\cdot \vecci{c_{j}})) \vecdi{d_{i}} -\beta\vecci{c_{j}} \right]$
 \EndFor 
 \EndWhile
 \\
 \Return $\matC$
\end{algorithmic}
\caption{Learning Category Vector Representations}
\label{alg:cat_embeddings}
\end{algorithm}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Advantages of Multinomial Logistic Regression Algorithm }

\begin{enumerate}
	\vfill\item<1-> Predicting relation between a document-category tuple is $\mathcal{O}(1)$
	\vfill\item<2-> Learns embeddings for categories in the same space as words and documents
	\vfill\item<3-> Though learns multiple category vectors, exploits the low-rank structure in the document-category relation
	\vfill\item<4-> Easy incorporation of additional relational data of documents for more accurate categorization as shown in \citet{gupta2015collectively}
	\vfill\item<5-> Usage of SGD makes algorithm completely online
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Performance Evaluation : Datasets }

\begin{enumerate}
	\vfill\item<1-> \textbf{Reuters-21578} : Standard dataset for categorization evaluation
	\vfill	\begin{table}[h!]
	%\tabcolsep=0.05cm
	\scriptsize
	\begin{center}
	\begin{tabular}{l c c c c c} % ccc ccc}
	\toprule
	& \textbf{$|\setD|$} & \textbf{$|\setC|$} & \textbf{$|\setW|$} & \textbf{Data Points} & \textbf{Sparsity}\\
	\midrule
	\textbf{Train Set}	& 7,767 & 90 & 39,853 & 9,585 & 0.0137 \\
	\textbf{Test Set}	& 3,019 & 90 & 39,853 & 3,745 & 0.0138 \\
	\bottomrule         
	\end{tabular}
	\end{center}
	\end{table}
	\vfill\item<2-> \textbf{Wikipedia Datasets} : Extracted for 4 top categories
	\vfill \begin{table}[h!]
	\scriptsize
	\begin{center}
	\begin{tabular}{l c c c c c} % ccc ccc}
	\toprule
	& \textbf{$|\setD|$} & \textbf{$|\setC|$} & \textbf{$|\setW|$} & \textbf{Data Points} & \textbf{Sparsity}\\
	\midrule
	\textbf{Physics}		& 4,229 & 2,999 & 81,614 & 14,070 & 0.0010 \\
	\textbf{Biology}		& 1,604 & 2,051 & 63,767 & 5,908 & 0.0018 \\
	\textbf{Sports}			& 1,529 & 2,829 & 59,058 & 3,745 & 0.0008 \\
	\textbf{Mathematics}	& 1,193 & 1,519 & 43,398 & 3,916 & 0.0013 \\
	\bottomrule         
	\end{tabular}
	\end{center}
	\end{table}
	\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Experimental Setup}
\begin{enumerate}
	\vfill\item<1-> \textbf{Evaluation Criteria} : Micro-averaged F1 score is used to evaluate performance. Micro-averaging considers all predictions equally across categories
	\vfill\item<2-> Capitalization in words is preserved
	\vfill\item<3-> Numbers are converted to `$\backslash num$'
	\vfill\item<4-> Words occurring less than $5$ times ignored
	\vfill\item<5-> Elements of document and word vectors are initialized by drawing uniformly from $[-\frac{1}{k}, \frac{1}{k}]$ 
	\vfill\item<6-> Hyper-parameters are fixed using performance on the validation set
	\vfill\item<7-> Noise Distribution for NCE is chosen as $P_{n}(w) \sim U(w)^{\frac{3}{4}}$
\end{enumerate}
\onslide<8->{
	\emph{For document categorization evaluation, $80\%$ of the documents are used for training and the rest are equally divided for test and validation purposes}
}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Baselines}
\begin{enumerate}
	\vfill\item<1-> \textbf{Bag-of-Words} : Most widely used representation with \emph{tf-idf} weighing
	\vfill\item<2-> \textbf{Latent Semantic Indexing} : Most effective dimensionality reduction technique for text
	\vfill\item<3-> \textbf{Word Vector Averaging} : Document representation by averaging word vectors with \emph{tf-idf} weighting
	\vfill\item<4-> \textbf{Probabilistic Matrix Factorization} : Simple matrix factorization of the document-category relation matrix
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Document Categorization Performance Evaluation Reuters-21578 }
\begin{columns}[T]
	\hspace{5mm}
	\begin{column}{.4\textwidth}
		\begin{table}[h!]
		\tabcolsep=0.1cm
		\scriptsize
		\begin{center}
		\begin{tabular}{l@{\hskip8mm} c c@{\hskip4mm} c}
		\toprule
		% & \multicolumn{3}{c}{Reuters-21578}         \\
		% \cmidrule(lr){2-4}
		\textbf{Reuters-21578} & {P} & {R} & \textbf{F1} \\
		\midrule
		\textbf{BOW}
		& 77.8   & 91.5  & 84.1 \\
		\textbf{LSI-100}
		& 84.8   & 96.7  & 90.4 \\
		\textbf{WordVecAvg}
		& 94.1   & 88.1  & 91.0 \\ \addlinespace[1mm]

		\textbf{SVM (poly)} \cite{joachims1998text}
		& -   & -  & 86.0 \\
		\textbf{SVM (rbf)} \cite{joachims1998text}
		& -   & -  & 86.4 \\ 
		\textbf{CMLF (CRF)} \cite{ghamrawi2005collective}
		& -   & -  & 87.0 \\
		\textbf{Binary-MFoM} \cite{gao2004mfom}
		& -   & -  & 88.4 \\ 
		\textbf{MC-MFoM} \cite{gao2004mfom}
		& -   & -  & 88.8 \\ 

		\addlinespace[1mm]
		\textbf{Our Model}
		& \multirow{2}{*}{92.1}   & \multirow{2}{*}{86.1}  & \multirow{2}{*}{89.0} \\
		(no weight) & & & \\ \addlinespace[1mm]
		\textbf{Our Model}
		& \multirow{2}{*}{94.1}   & \multirow{2}{*}{89.3}  & \multirow{2}{*}{\highest{91.7}} \\
		(with weights) & & & \\
		\bottomrule         
		\end{tabular}
		%\vskip -4mm
		\caption*{\footnotesize Precision/Recall/F1 for Document Categorization on Reuters-21578 }
		\end{center}
		\end{table}
	\end{column}
	%\hspace{5mm}
\onslide<2->{	
	\begin{column}{.6\textwidth}
	\begin{center}
		\begin{figure}[tb]
		\centering
			\includegraphics[scale=0.45]{../figs/pr/reuter-cs-scala.pdf}
		\end{figure}
	\end{center}
	\end{column}
}	
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Document Categorization Performance Evaluation \\ Physics - Wikipedia }
\begin{columns}[T]
	\hspace{5mm}
	\begin{column}{.4\textwidth}
		\begin{table}[h!]
		\tabcolsep=0.1cm
		\scriptsize
		\begin{center}
		\begin{tabular}{l@{\hskip5mm} c c@{\hskip4mm} c}
		\toprule
		% & \multicolumn{3}{c}{Reuters-21578}         \\
		% \cmidrule(lr){2-4}
		\textbf{Physics (Wikipedia)} & {P} & {R} & \textbf{F1} \\
		\midrule
		\textbf{BOW}
		& 87.8   & 70.1  & 77.9 \\
		\textbf{LSI-100}
		& 83.4   & 69.5  & 75.8 \\
		\textbf{WordVecAvg}
		& 91.0   & 59.1  & 71.7 \\ \addlinespace[1mm]

		\textbf{Our Model}
		& \multirow{2}{*}{86.1}   & \multirow{2}{*}{64.6}  & \multirow{2}{*}{73.8} \\
		(no weights) & & & \\ \addlinespace[1mm]
		\textbf{Our Model}
		& \multirow{2}{*}{88.6}   & \multirow{2}{*}{72.4}  & \multirow{2}{*}{\highest{79.7}} \\
		(with weights) & & & \\
		\bottomrule         
		\end{tabular}
		\caption*{\footnotesize Precision/Recall/F1 for Document Categorization on Physics dataset}
		\end{center}
		\end{table}
	\end{column}
	%\hspace{5mm}
\onslide<2->{	
	\begin{column}{.6\textwidth}
	\begin{center}
		\begin{figure}[tb]
		\centering
			\includegraphics[scale=0.45]{../figs/pr/physics-cs-scala.pdf}
		\end{figure}
	\end{center}
	\end{column}
}	
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Document Categorization Performance Evaluation \\ Biology - Wikipedia }
\begin{columns}[T]
	\hspace{5mm}
	\begin{column}{.4\textwidth}
		\begin{table}[h!]
		\tabcolsep=0.1cm
		\scriptsize
		\begin{center}
		\begin{tabular}{l@{\hskip5mm} c c@{\hskip4mm} c}
		\toprule
		% & \multicolumn{3}{c}{Reuters-21578}         \\
		% \cmidrule(lr){2-4}
		\textbf{Biology (Wikipedia)} & {P} & {R} & \textbf{F1} \\
		\midrule
		\textbf{BOW}
		& 90.3   & 59.5  & \highest{69.0} \\
		\textbf{LSI-100}
		& 82.1   & 51.6  & 63.4 \\
		\textbf{WordVecAvg}
		& 79.4   & 50.4  & 61.6 \\ \addlinespace[1mm]

		\textbf{Our Model}
		& \multirow{2}{*}{80.3}   & \multirow{2}{*}{53.8}  & \multirow{2}{*}{64.4} \\
		(no weights) & & & \\ \addlinespace[1mm]
		\textbf{Our Model}
		& \multirow{2}{*}{79.7}   & \multirow{2}{*}{59.0}  & \multirow{2}{*}{67.8} \\
		(with weights) & & & \\
		\bottomrule         
		\end{tabular}
		\caption*{\footnotesize Precision/Recall/F1 for Document Categorization on Biology dataset}
		\end{center}
		\end{table}
	\end{column}
	%\hspace{5mm}
\onslide<2->{	
	\begin{column}{.6\textwidth}
	\begin{center}
		\begin{figure}[tb]
		\centering
			\includegraphics[scale=0.45]{../figs/pr/biology-cs-scala.pdf}
		\end{figure}
	\end{center}
	\end{column}
}	
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Document Categorization Performance Evaluation \\ Mathematics - Wikipedia }
\begin{columns}[T]
	\hspace{5mm}
	\begin{column}{.4\textwidth}
		\begin{table}[h!]
	\tabcolsep=0.1cm
	\scriptsize
	\begin{center}
	\begin{tabular}{l@{\hskip3mm} c c@{\hskip4mm} c}
	\toprule
	% & \multicolumn{3}{c}{Reuters-21578}         \\
	% \cmidrule(lr){2-4}
	\textbf{Mathematics (Wikipedia)} & {P} & {R} & \textbf{F1} \\
	\midrule
	\textbf{BOW}
	& 65.6   & 65.1  & 65.3 \\
	\textbf{LSI-100}
	& 89.7   & 50.3  & 64.4 \\
	\textbf{WordVecAvg}
	& 90.5   & 40.3  & 55.7 \\ \addlinespace[1mm]

	\textbf{Our Model}
	& \multirow{2}{*}{78.4}   & \multirow{2}{*}{57.4}  & \multirow{2}{*}{66.3} \\
	(no weights) & & & \\ \addlinespace[1mm]
	\textbf{Our Model}
	& \multirow{2}{*}{85.3}   & \multirow{2}{*}{56.8}  & \multirow{2}{*}{\highest{68.2}} \\
	(with weights) & & & \\
	\bottomrule         
	\end{tabular}
	%\vskip -4mm
	\caption*{\footnotesize Precision/Recall/F1 for Document Categorization on Mathematics dataset}
	\end{center}
	\end{table}
	\end{column}
	%\hspace{5mm}
\onslide<2->{	
	\begin{column}{.6\textwidth}
	\begin{center}
		\begin{figure}[tb]
		\centering
			\includegraphics[scale=0.45]{../figs/pr/mathematics-cs-scala.pdf}
		\end{figure}
	\end{center}
	\end{column}
}	
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Document Categorization Performance Evaluation \\ Sports - Wikipedia }
\begin{columns}[T]
	\hspace{5mm}
	\begin{column}{.4\textwidth}
		\vfill
		\begin{table}[h!]
		\tabcolsep=0.1cm
		\scriptsize
		\begin{center}
		\begin{tabular}{l@{\hskip5mm} c c@{\hskip4mm} c}
		\toprule
		% & \multicolumn{3}{c}{Reuters-21578}         \\
		% \cmidrule(lr){2-4}
		\textbf{Sports (Wikipedia)} & {P} & {R} & \textbf{F1} \\
		\midrule
		\textbf{BOW}
		& 91.7   & 41.3  & 56.9 \\
		\textbf{LSI-100}
		& 91.2   & 40.1  & 55.7 \\
		\textbf{WordVecAvg}
		& 81.8   & 37.5  & 51.4 \\ \addlinespace[1mm]

		\textbf{Our Model}
		& \multirow{2}{*}{80.5}   & \multirow{2}{*}{40.1}  & \multirow{2}{*}{53.6} \\
		(no weights) & & & \\ \addlinespace[1mm]
		\textbf{Our Model}
		& \multirow{2}{*}{82.1}   & \multirow{2}{*}{44.0}  & \multirow{2}{*}{\highest{57.3}} \\
		(with weights) & & & \\
		\bottomrule         
		\end{tabular}
		%\vskip -4mm
		\caption*{\footnotesize Precision/Recall/F1 for Document Categorization on Sports dataset}
		\end{center}
		\end{table}
	\end{column}
	%\hspace{5mm}
	
\onslide<2->{	
	\begin{column}{.6\textwidth}
	\vfill
	\begin{center}
		\begin{figure}[tb]
		\centering
			\includegraphics[scale=0.45]{../figs/pr/sports-cs-scala.pdf}
		\end{figure}
	\end{center}
	\end{column}
}	
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Imputing Missing Categories in Wikipedia }
\begin{enumerate}
	\vfill\item Real-life databases contain missing information
	\vfill\item Wikipedia is a large-scale database with non-expert annotators
\end{enumerate}
\vfill
We evaluate our model on imputing missing categories in the Wikipedia datasets
\vfill
	\begin{table}[h!]
	\tabcolsep=1mm
	\tiny
	\begin{center}
	%\begin{tabular}{l cc@{\hskip 2mm}c@{\hskip 2mm} cc@{\hskip 2mm}c@{\hskip 2mm} cc@{\hskip 2mm}c@{\hskip 2mm} cc@{\hskip 2mm}c@{\hskip 2mm} cc@{\hskip 2mm}c}
	\begin{tabular}{l ccc@{\hskip 3mm} ccc@{\hskip 3mm} ccc@{\hskip 3mm} ccc@{\hskip 3mm} ccc}
	\toprule
	\multirow{2}{*}{} & 
	\multicolumn{3}{c}{{Physics}}         & 
	\multicolumn{3}{c}{{Biology}}        & \multicolumn{3}{c}{{Mathematics}}         & \multicolumn{3}{c}{{Sports}}        & %\multicolumn{3}{c}{\textbf{Restaurant}}       & 
	\multicolumn{3}{c}{\textbf{Combined}}              
	\\ 
	\cmidrule(lr){2-4}
	\cmidrule(lr){5-7}
	\cmidrule(lr){8-10}
	\cmidrule(lr){11-13}
	\cmidrule(lr){14-16}
	& 
	{P} & {R} & \textbf{F1} & 
	{P} & {R} & \textbf{F1} & 
	{P} & {R} & \textbf{F1} & 
	{P} & {R} & \textbf{F1} &
	{P} & {R} & \textbf{F1} \\ 
	\midrule
	\textbf{PMF}
	& 73.0   & 64.3  & 68.4
	& 72.1   & 47.5  & 57.3
	& 41.6   & 58.2  & 48.5
	& 51.3   & 35.6  & 42.0
	& 63.0   & 54.8  & 58.6 
	\\
	\textbf{LSI-100}
	& 59.5   & 82.3  & 69.0
	& 49.9   & 71.6  & 58.8
	& 47.1   & 73.0  & 57.3
	& 43.1   & 68.2  & 52.8
	& 52.5   & 76.3  & 62.2
	\\ 
	\textbf{BOW}
	& 76.1   & 79.4  & 77.7
	& 69.7   & 67.7  & 68.7
	& 70.9   & 63.5  & 67.0
	& 64.8   & 49.3  & 56.0
	& 72.5   & 69.4  & 70.9
	\\
	\textbf{WordVecAvg}
	& 88.0   & 63.5  & 73.8
	& 80.7   & 50.3  & 61.9
	& 71.8   & 46.7  & 56.6
	& 87.2   & 35.4  & 50.3
	& 84.2   & 53.4  & 65.4
	\\ \addlinespace[1mm]
	\textbf{Our Model}
	& \multirow{2}{*}{88.6}   & \multirow{2}{*}{69.1}  & \multirow{2}{*}{77.7}
	& \multirow{2}{*}{80.5}   & \multirow{2}{*}{55.3}  & \multirow{2}{*}{65.6}
	& \multirow{2}{*}{74.3}   & \multirow{2}{*}{53.1}  & \multirow{2}{*}{61.9}
	& \multirow{2}{*}{84.7}   & \multirow{2}{*}{40.2}  & \multirow{2}{*}{54.5}
	& \multirow{2}{*}{85.4}   & \multirow{2}{*}{58.5}  & \multirow{2}{*}{69.2}
	\\ 
	(without weights) & & & & & & & & & & & & & &  & \\
	\addlinespace[1mm]
	\textbf{Our Model}
	& \multirow{2}{*}{89.9}   & \multirow{2}{*}{74.5}  & \multirow{2}{*}{\highest{81.5}}
	& \multirow{2}{*}{84.9}   & \multirow{2}{*}{63.8} & \multirow{2}{*}{\highest{72.9}}
	& \multirow{2}{*}{79.9}   & \multirow{2}{*}{60.7}  & \multirow{2}{*}{\highest{69.0}}
	& \multirow{2}{*}{81.1}   & \multirow{2}{*}{45.6} & \multirow{2}{*}{\highest{58.4}}
	& \multirow{2}{*}{86.3}   & \multirow{2}{*}{65.2}  & \multirow{2}{*}{\highest{74.3}}
	\\ 
	(with weights) & & & & & & & & & & & & & &  & \\
	\bottomrule         
	\end{tabular}
	\end{center}
	\end{table}


\end{frame}

\begin{frame}{Estimating Similarity between Categories and Words }
\begin{enumerate}
	\vfill\item We embed words, document and categories in the same $k$-dimensional space
	\vfill\item This allows us to estimate similarity between entities non directly related
\end{enumerate}
\begin{table}[h!]
\tabcolsep=1mm
\scriptsize
\begin{center}
%\begin{adjustwidth}{-1.5cm}{}
\begin{tabular}{l@{\hskip3mm} l}
\toprule
\multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{Nearest Neighbors}} \\
 & \\
%\cmidrule{2-2}
\textbf{Evolutionary Biology}   & gene, phylogenetics, speciation, ancestor, Darwin, lineage, evolutionary, interbreeding \\
\textbf{Statistical Mechanics}  & ergodicity, Eigenstate, Universality, DMFT, Markovian, Parisi, Combinatorics \\
\textbf{Thermodynamics}         & Convection, ecosystem, Enthalpy, Joule, calorimetric, compressible, Thermodynamic \\
\textbf{Trade}                  & import, Pledges, Tariff, Trade, competitiveness, toll, billion, basket, Ditch, Worldwide \\
\textbf{Money-FX}               & Borrowing, franc, banker, Currency, banks, nervous, sideways, Markets, FORWARD \\
\textbf{Virology}               & nucleoside, ribozyme, adenoviruses, Virology, retroviruses, poliovirus, Viroid \\
\textbf{Neurobiology}           & purinergic, cyclase, vertebral, Ehrlich, nexus, steroid, lean, gendered, reticular \\
\textbf{Physical Exercise}      & Fitness, aerobics, metabolic, workout, Exercise, Stretching, pelvic, Physiology, fibers \\
\textbf{Algebra}                & subalgebra, Algebras, nilpotent, adjoints, octonions, bicommutant, diagonalizable \\
\textbf{Theoretical Physicists} & Dipankar, DSc, Hubert, Aneesur, Uri, Ignaz, Chia, Stig, Diderot, Dannie \\
\textbf{Mathematical Physics}   & covectors, pseudotensor, spacelike, dyadic, Curl, torque, contractions, wavefunctions \\
\textbf{Sports Venues}          & stadion, decoration, tracks, seating, buildings, parcourse, architectural, arenas, circular \\
\textbf{Indian Mathematics}     & utkrama, ecliptic, Siddhanta, Hellenistic, Brahmi, sexagesimal, scribe, Islamic, Sanskrit \\
\bottomrule         
\end{tabular}
 %\end{adjustwidth}
\end{center}
%\caption{\label{catword:sim} Estimating Similarity between Categories and Words}
\end{table}
\end{frame}

\begin{frame}{Conclusion }
\begin{enumerate}
	\vfill\item<1-> We presented an unsupervised neural network model that 
	\begin{itemize}
		\vfill\item Jointly learns fixed-length low-dimensional distributed vector representations for documents and words
		\vfill\item Encode semantic content of words and documents in these representations
	\end{itemize}
	
	\vfill\item<2-> We overcome some of the problems with the bag-of-words representations 
	\vfill\item<3-> Our model is a log-linear model that uses NCE
	\vfill\item<4-> We improve state-of-the-art results on multi-label document categorization
	\begin{itemize}
		\vfill\item On the Reuters-21578 dataset we improve by $3.26\%$
		\vfill\item On the Reuters-21578 dataset we improve over BOW by $9\%$
	\end{itemize}
	\vfill\item<5-> We show the best performance on imputing missing categories in Wikipedia
	\vfill\item<6-> Learned distributed representations allow semantic similarity estimation
\end{enumerate}
\end{frame}

\begin{frame}{Future Work }
\begin{enumerate}
	\item<1-> Improving compositionality of Word Vectors
	\vfill\item<2-> Joint Document Representation Learning and Document Categorization
	\vfill\item<3-> Supervised Multi-view Relational Learning
\end{enumerate}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]{References}
    %{\footnotesize
    \scriptsize
    \bibliographystyle{abbrvnat}
    \bibliography{references}
    %}
\end{frame}


\end{document}