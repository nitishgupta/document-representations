\contentsline {chapter}{Abstract}{i}{figure.caption.1}
\contentsline {chapter}{List of Tables}{vii}{chapter*.4}
\contentsline {chapter}{List of Figures}{ix}{chapter*.5}
\contentsline {chapter}{List of Algorithms}{xi}{chapter*.6}
\contentsline {chapter}{\numberline {1}Related Work}{1}{chapter.7}
\contentsline {section}{\numberline {1.1}Text Representation}{1}{section.8}
\contentsline {subsection}{\numberline {1.1.1}Bag of Words}{2}{subsection.9}
\contentsline {subsection}{\numberline {1.1.2}Dimensionality Reduction / Feature Selection}{3}{subsection.15}
\contentsline {section}{\numberline {1.2}Learning Algorithms}{5}{section.24}
\contentsline {subsection}{\numberline {1.2.1}With Multiple Binary Classifiers}{6}{subsection.25}
\contentsline {subsection}{\numberline {1.2.2}With Single Joint Classifier}{9}{subsection.30}
\contentsline {chapter}{\numberline {2}Distributed Document Embeddings}{13}{chapter.35}
\contentsline {section}{\numberline {2.1}Motivation}{13}{section.36}
\contentsline {section}{\numberline {2.2}Background on Word Embeddings}{14}{section.39}
\contentsline {subsection}{\numberline {2.2.1}Neural Probabilistic Language Model (NPLM)}{15}{subsection.40}
\contentsline {subsection}{\numberline {2.2.2}Log-Linear Models : word2vec}{17}{subsection.47}
\contentsline {subsubsection}{\numberline {2.2.2.1}Continuous Bag-of-Words (CBOW)}{17}{subsubsection.48}
\contentsline {subsubsection}{\numberline {2.2.2.2}Continuous Skip-gram}{18}{subsubsection.53}
\contentsline {subsubsection}{\numberline {2.2.2.3}Dependency-based Word Embeddings}{19}{subsubsection.56}
\contentsline {section}{\numberline {2.3}Document Embeddings}{20}{section.58}
\contentsline {subsection}{\numberline {2.3.1}Problem Setup}{21}{subsection.61}
\contentsline {subsection}{\numberline {2.3.2}Our Model}{22}{subsection.62}
\contentsline {subsubsection}{\numberline {2.3.2.1}Projection Layer (Context Representation)}{23}{subsubsection.67}
\contentsline {subsubsection}{\numberline {2.3.2.2}Estimating Prediction Probability}{24}{subsubsection.69}
\contentsline {subsubsection}{\numberline {2.3.2.3}Training Objective}{24}{subsubsection.74}
\contentsline {subsubsection}{\numberline {2.3.2.4}Noise Contrastive Estimation (NCE)}{25}{subsubsection.78}
\contentsline {subsubsection}{\numberline {2.3.2.5}New Training Objective}{27}{subsubsection.82}
\contentsline {subsubsection}{\numberline {2.3.2.6}Parameter Estimation}{27}{subsubsection.86}
\contentsline {subsubsection}{\numberline {2.3.2.7}Hyper-parameters}{30}{subsubsection.112}
\contentsline {chapter}{\numberline {3}Multi-Label Text Categorization}{33}{chapter.119}
\contentsline {section}{\numberline {3.1}Logistic Regression (LR) for Multi-label Document Categorization}{33}{section.120}
\contentsline {subsection}{\numberline {3.1.1}Training Data}{34}{subsection.121}
\contentsline {subsection}{\numberline {3.1.2}Logistic Regression Model}{35}{subsection.122}
\contentsline {subsubsection}{\numberline {3.1.2.1}Training Objective}{35}{subsubsection.126}
\contentsline {subsubsection}{\numberline {3.1.2.2}Parameter Estimation}{36}{subsubsection.133}
\contentsline {section}{\numberline {3.2}Similarity to Relational Learning}{38}{section.140}
\contentsline {section}{\numberline {3.3}Advantages of Logistic Regression Learning Algorithm}{39}{section.144}
\contentsline {chapter}{\numberline {4}Datasets and Evaluations}{41}{chapter.149}
\contentsline {section}{\numberline {4.1}Datasets}{41}{section.150}
\contentsline {subsection}{\numberline {4.1.1}Reuters-21578}{42}{subsection.151}
\contentsline {subsection}{\numberline {4.1.2}Wikipedia Datasets}{43}{subsection.153}
\contentsline {section}{\numberline {4.2}Experimental Setup}{44}{section.156}
\contentsline {chapter}{Bibliography}{49}{Item.160}
