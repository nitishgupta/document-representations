\BOOKMARK [0][-]{figure.caption.1}{Abstract}{}% 1
\BOOKMARK [0][-]{chapter*.4}{List of Tables}{}% 2
\BOOKMARK [0][-]{chapter*.5}{List of Figures}{}% 3
\BOOKMARK [0][-]{chapter*.6}{List of Algorithms}{}% 4
\BOOKMARK [0][-]{chapter.7}{1 Related Work}{}% 5
\BOOKMARK [1][-]{section.8}{1.1 Text Representation}{chapter.7}% 6
\BOOKMARK [2][-]{subsection.9}{1.1.1 Bag of Words}{section.8}% 7
\BOOKMARK [2][-]{subsection.15}{1.1.2 Dimensionality Reduction / Feature Selection}{section.8}% 8
\BOOKMARK [1][-]{section.24}{1.2 Learning Algorithms}{chapter.7}% 9
\BOOKMARK [2][-]{subsection.25}{1.2.1 With Multiple Binary Classifiers}{section.24}% 10
\BOOKMARK [2][-]{subsection.30}{1.2.2 With Single Joint Classifier}{section.24}% 11
\BOOKMARK [0][-]{chapter.35}{2 Distributed Document Embeddings}{}% 12
\BOOKMARK [1][-]{section.36}{2.1 Motivation}{chapter.35}% 13
\BOOKMARK [1][-]{section.39}{2.2 Background on Word Embeddings}{chapter.35}% 14
\BOOKMARK [2][-]{subsection.40}{2.2.1 Neural Probabilistic Language Model \(NPLM\)}{section.39}% 15
\BOOKMARK [2][-]{subsection.47}{2.2.2 Log-Linear Models : word2vec}{section.39}% 16
\BOOKMARK [3][-]{subsubsection.48}{2.2.2.1 Continuous Bag-of-Words \(CBOW\)}{subsection.47}% 17
\BOOKMARK [3][-]{subsubsection.53}{2.2.2.2 Continuous Skip-gram}{subsection.47}% 18
\BOOKMARK [3][-]{subsubsection.56}{2.2.2.3 Dependency-based Word Embeddings}{subsection.47}% 19
\BOOKMARK [1][-]{section.58}{2.3 Document Embeddings}{chapter.35}% 20
\BOOKMARK [2][-]{subsection.61}{2.3.1 Problem Setup}{section.58}% 21
\BOOKMARK [2][-]{subsection.62}{2.3.2 Our Model}{section.58}% 22
\BOOKMARK [3][-]{subsubsection.67}{2.3.2.1 Projection Layer \(Context Representation\)}{subsection.62}% 23
\BOOKMARK [3][-]{subsubsection.69}{2.3.2.2 Estimating Prediction Probability}{subsection.62}% 24
\BOOKMARK [3][-]{subsubsection.74}{2.3.2.3 Training Objective}{subsection.62}% 25
\BOOKMARK [3][-]{subsubsection.78}{2.3.2.4 Noise Contrastive Estimation \(NCE\)}{subsection.62}% 26
\BOOKMARK [3][-]{subsubsection.82}{2.3.2.5 New Training Objective}{subsection.62}% 27
\BOOKMARK [3][-]{subsubsection.86}{2.3.2.6 Parameter Estimation}{subsection.62}% 28
\BOOKMARK [3][-]{subsubsection.112}{2.3.2.7 Hyper-parameters}{subsection.62}% 29
\BOOKMARK [0][-]{chapter.117}{3 Multi-Label Text Categorization}{}% 30
\BOOKMARK [1][-]{section.118}{3.1 Logistic Regression \(LR\) for Multi-label Document Categorization}{chapter.117}% 31
\BOOKMARK [2][-]{subsection.119}{3.1.1 Training Data}{section.118}% 32
\BOOKMARK [2][-]{subsection.120}{3.1.2 Logistic Regression Model}{section.118}% 33
\BOOKMARK [3][-]{subsubsection.124}{3.1.2.1 Training Objective}{subsection.120}% 34
\BOOKMARK [3][-]{subsubsection.131}{3.1.2.2 Parameter Estimation}{subsection.120}% 35
\BOOKMARK [1][-]{section.138}{3.2 Similarity to Relational Learning}{chapter.117}% 36
\BOOKMARK [1][-]{section.142}{3.3 Advantages of Logistic Regression Learning Algorithm}{chapter.117}% 37
\BOOKMARK [0][-]{chapter.147}{4 Datasets and Evaluations}{}% 38
\BOOKMARK [1][-]{section.148}{4.1 Datasets}{chapter.147}% 39
\BOOKMARK [2][-]{subsection.149}{4.1.1 Reuters-21578}{section.148}% 40
\BOOKMARK [2][-]{subsection.151}{4.1.2 Wikipedia Datasets}{section.148}% 41
\BOOKMARK [0][-]{table.caption.153}{Bibliography}{}% 42
