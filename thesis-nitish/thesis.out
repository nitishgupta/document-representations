\BOOKMARK [0][-]{figure.caption.1}{Abstract}{}% 1
\BOOKMARK [0][-]{chapter*.4}{List of Tables}{}% 2
\BOOKMARK [0][-]{chapter*.5}{List of Figures}{}% 3
\BOOKMARK [0][-]{chapter*.6}{List of Algorithms}{}% 4
\BOOKMARK [0][-]{chapter.7}{1 Introduction}{}% 5
\BOOKMARK [1][-]{section.9}{1.1 Motivation}{chapter.7}% 6
\BOOKMARK [2][-]{subsection.10}{1.1.1 Inability to preserve word ordering}{section.9}% 7
\BOOKMARK [2][-]{subsection.11}{1.1.2 Lack of similarity measures}{section.9}% 8
\BOOKMARK [2][-]{subsection.12}{1.1.3 Compositionality of distributed word vectors}{section.9}% 9
\BOOKMARK [1][-]{section.13}{1.2 Problem Statement}{chapter.7}% 10
\BOOKMARK [1][-]{section.15}{1.3 Organization of Thesis}{chapter.7}% 11
\BOOKMARK [0][-]{chapter.16}{2 Background on Document Categorization}{}% 12
\BOOKMARK [1][-]{section.17}{2.1 Text Representation}{chapter.16}% 13
\BOOKMARK [2][-]{subsection.18}{2.1.1 Bag of Words}{section.17}% 14
\BOOKMARK [2][-]{subsection.20}{2.1.2 Dimensionality Reduction and Feature Selection}{section.17}% 15
\BOOKMARK [1][-]{section.25}{2.2 Learning Algorithms}{chapter.16}% 16
\BOOKMARK [2][-]{subsection.26}{2.2.1 Document Categorization using Binary Classifiers}{section.25}% 17
\BOOKMARK [2][-]{subsection.27}{2.2.2 Document Categorization with Single Joint Classifier}{section.25}% 18
\BOOKMARK [0][-]{chapter.29}{3 Distributed Document Representations}{}% 19
\BOOKMARK [1][-]{section.30}{3.1 Need for Distributed Word Representations}{chapter.29}% 20
\BOOKMARK [1][-]{section.33}{3.2 Background on Word Embeddings}{chapter.29}% 21
\BOOKMARK [2][-]{subsection.34}{3.2.1 Neural Probabilistic Language Model}{section.33}% 22
\BOOKMARK [2][-]{subsection.41}{3.2.2 Log-Linear Models}{section.33}% 23
\BOOKMARK [3][-]{subsubsection.42}{3.2.2.1 Continuous Bag-of-Words Model}{subsection.41}% 24
\BOOKMARK [3][-]{subsubsection.47}{3.2.2.2 Continuous Skip-gram Model}{subsection.41}% 25
\BOOKMARK [3][-]{subsubsection.50}{3.2.2.3 Dependency-based Word Embeddings}{subsection.41}% 26
\BOOKMARK [1][-]{section.52}{3.3 Document Representation}{chapter.29}% 27
\BOOKMARK [2][-]{subsection.55}{3.3.1 Problem Setup}{section.52}% 28
\BOOKMARK [2][-]{subsection.56}{3.3.2 Our Model}{section.52}% 29
\BOOKMARK [3][-]{subsubsection.61}{3.3.2.1 Context Representation}{subsection.56}% 30
\BOOKMARK [3][-]{subsubsection.63}{3.3.2.2 Estimating Prediction Probability}{subsection.56}% 31
\BOOKMARK [3][-]{subsubsection.68}{3.3.2.3 Learning Objective}{subsection.56}% 32
\BOOKMARK [3][-]{subsubsection.72}{3.3.2.4 Noise Contrastive Estimation}{subsection.56}% 33
\BOOKMARK [3][-]{subsubsection.76}{3.3.2.5 Learning Objective using NCE}{subsection.56}% 34
\BOOKMARK [3][-]{subsubsection.80}{3.3.2.6 Parameter Estimation}{subsection.56}% 35
\BOOKMARK [3][-]{subsubsection.107}{3.3.2.7 Hyper-parameters of our model}{subsection.56}% 36
\BOOKMARK [0][-]{chapter.114}{4 Multi-Label Document Categorization}{}% 37
\BOOKMARK [1][-]{section.115}{4.1 Logistic Regression for Multi-label Document Categorization}{chapter.114}% 38
\BOOKMARK [2][-]{subsection.116}{4.1.1 Training Data}{section.115}% 39
\BOOKMARK [2][-]{subsection.117}{4.1.2 Logistic Regression Model}{section.115}% 40
\BOOKMARK [3][-]{subsubsection.121}{4.1.2.1 Training Objective}{subsection.117}% 41
\BOOKMARK [3][-]{subsubsection.128}{4.1.2.2 Parameter Estimation}{subsection.117}% 42
\BOOKMARK [1][-]{section.136}{4.2 Similarity to Relational Learning}{chapter.114}% 43
\BOOKMARK [1][-]{section.140}{4.3 Advantages of Logistic Regression Learning Algorithm}{chapter.114}% 44
\BOOKMARK [0][-]{chapter.145}{5 Datasets and Evaluations}{}% 45
\BOOKMARK [1][-]{section.146}{5.1 Datasets}{chapter.145}% 46
\BOOKMARK [2][-]{subsection.147}{5.1.1 Reuters-21578}{section.146}% 47
\BOOKMARK [2][-]{subsection.149}{5.1.2 Wikipedia Datasets}{section.146}% 48
\BOOKMARK [1][-]{section.152}{5.2 Experimental Setup}{chapter.145}% 49
\BOOKMARK [1][-]{section.157}{5.3 Results}{chapter.145}% 50
\BOOKMARK [2][-]{subsection.158}{5.3.1 Document Categorization}{section.157}% 51
\BOOKMARK [3][-]{subsubsection.159}{5.3.1.1 Reuters - 21578}{subsection.158}% 52
\BOOKMARK [3][-]{subsubsection.166}{5.3.1.2 Physics - Wikipedia}{subsection.158}% 53
\BOOKMARK [3][-]{subsubsection.173}{5.3.1.3 Biology - Wikipedia}{subsection.158}% 54
\BOOKMARK [3][-]{subsubsection.180}{5.3.1.4 Mathematics - Wikipedia}{subsection.158}% 55
\BOOKMARK [3][-]{subsubsection.187}{5.3.1.5 Sports - Wikipedia}{subsection.158}% 56
\BOOKMARK [2][-]{subsection.194}{5.3.2 Imputing Missing Categories}{section.157}% 57
\BOOKMARK [2][-]{subsection.196}{5.3.3 Estimating Similarity between Categories and Words}{section.157}% 58
\BOOKMARK [0][-]{chapter.198}{6 Conclusions and Future Work}{}% 59
\BOOKMARK [1][-]{section.199}{6.1 Future Work}{chapter.198}% 60
\BOOKMARK [2][-]{subsection.200}{6.1.1 Improving Compositionality of Word Vectors}{section.199}% 61
\BOOKMARK [2][-]{subsection.201}{6.1.2 Multi-view Supervised Representation Learning}{section.199}% 62
\BOOKMARK [0][-]{subsection.201}{Bibliography}{}% 63
