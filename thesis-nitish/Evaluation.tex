\chapter{Datasets and Evaluations}
\label{chapter:evaluations}
\todo{Section 1 : Datasets, two types. Subsection 1 : Wikipedia Datasets. Relative Sizes. Subsection 2 : Reuters Dataset. ModApte Split. Num of categories. Docs.}

\todo{Section 2 : Experimental Setup. How is evaluation done. Hyper-parameters. Baselines }

\todo{Section 3 : Document Categorization}

\todo{Section 4 : Imputing Missing Categories. Only done on Wikipedia datasets}

\todo{Section 5 : Estimating Similarity between cats, docs, words}

In this chapter we first introduce the datasets that we use for evaluating the efficacy of our document representations for the document categorization task. We then explain the details of our experimentation setup and the different document representation techniques that form strong baselines for the task. In the sections following that we present the results for assigning categories to new documents and also recommending categories for documents which we already have some prior category information. 

\section{Datasets}
We perform our experimentation on 5 datasets that contain rich data about documents belonging to multiple categories simultaneously. One of the 5 datasets we use is the famous Reuters-21578 collection, which is considered the benchmark for text classification evaluation. Along with the being richly multi-label and large-sized, Reuters-21578 has been used for many years for evaluation which gives us the opportunity to compare our accuracy with the previous state-of-the-art results. The other 4 datasets that we evaluate on are, exclusive subsets of documents, extracted from Wikipedia.

\subsection{Reuters-21578}






