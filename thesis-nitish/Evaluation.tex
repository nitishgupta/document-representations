\chapter{Datasets and Evaluations}
\label{chapter:evaluations}
\todo{Section 3 : Results - Document Categorization
Section 4 : Results - Imputing Missing Categories. Only done on Wikipedia datasets
Section 5 : Estimating Similarity between cats, docs, words}

In this chapter we first introduce the datasets that we use for evaluating the efficacy of our document representations for the document categorization task. We then explain the details of our experimentation setup and the different document representation techniques that form strong baselines for the task. In the sections following that we present the results for assigning categories to new documents and also recommending categories for documents which we already have some prior category information. 

\section{Datasets}
We perform our experimentation on 5 datasets that contain rich data about documents belonging to multiple categories simultaneously. One of the 5 datasets we use is the famous Reuters-21578 collection, which is considered the benchmark for text classification evaluation. Along with the being richly multi-label and large-sized, Reuters-21578 has been used for many years for evaluation which gives us the opportunity to compare our accuracy with the previous state-of-the-art results. The other 4 datasets that we evaluate on are, exclusive subsets of documents, extracted from Wikipedia.


\subsection{Reuters-21578}
Reuters-21578 collection consists of documents published on the Reuters newswire in 1987. As the name suggests it has a total of 21758 documents and contains a total of 135 categories. Though most of the documents in the collection are multi-labeled, many documents are assigned only a single category. The Reuters-21578 dataset has over the years become a standard dataset for evaluating many information retrieval algorithms due to the the multi-label nature of the collection and the large number of categories present in the collection that are overlapping and non-exhaustive. Relationships between the categories also makes it an interesting dataset to evaluate on, as the algorithms that capture the correlations between the categories are bound to perform better. Even though the large number of documents and categories present, the dataset is very sparse that makes learning difficult.

Though there exist many processed versions of the collection, the \emph{ModApte} (Modified Apte) version is the most widely used version of the Reuters-21578 for evaluating multi-label classification algorithms. The \emph{ModApte} version predefines a train/test split by considering all documents published after a specific date for testing purposes and the rest for training. After the split, only categories considered are the ones that have atleast one document in the training and the test set. The number of documents, categories, words and other statistics of the Reuters(\emph{ModApte}) dataset are given in Table~\ref{reuter:data:stat}.

\begin{table}[h!]
%\tabcolsep=0.05cm
%\footnotesize
\begin{center}
\begin{tabular}{l c c c c c} % ccc ccc}
\toprule
& \textbf{$|\setD|$} & \textbf{$|\setC|$} & \textbf{$|\setW|$} & \textbf{Data Points} & \textbf{Sparsity}\\
\midrule
\textbf{Train Set}	& 7\,767 & 90 & 39\,853 & 9\,585 & 0.0137 \\
\textbf{Test Set}	& 3\,019 & 90 & 39\,853 & 3\,745 & 0.0138 \\
\bottomrule         
\end{tabular}
\caption{\label{reuter:data:stat}Statistics of the Reuters-21578 (\emph{ModApte}) Dataset}
\end{center}
\end{table}

\subsection{Wikipedia Datasets}
Wikipedia\footnote{www.wikipedia.org} is a free-access free content Internet encyclopedia that contains articles about virtually anything possible. Along with the humongous amounts of articles, Wikipedia also has a hierarchical cyclic \emph{Wikipedia Category Graph} that is used to label articles with the categories they belong in. 
Though the category graph is completely connected, it has few major top-level categories within which all subsequent categories fall. For eg. some of the top-level categories are \emph{Culture and Arts}, \emph{Geography}, \emph{Health}, \emph{History}, \emph{Mathematics}, \emph{Natural and Physical Sciences}, \emph{Philosophy} etc. Each of the top-level categories are further divided into deep trees of fine-grained categories that are assigned to the articles. 

The categories that are assigned to an article thus ranges from broader categories to much fine-grained that are very difficult to assign via an automated system. For eg. some of the categories assigned to an article on the musician \emph{Jimi Hendrix} are \emph{1942 Births}, \emph{1970 deaths}, \emph{American Roch Guitarists}, \emph{Musicians from Seattle}, \emph{Military Brats}, \emph{Alcohol-related deaths in England}. Automatic categorization of such granularity requires the document representation to capture the different semantic topics in the document with great accuracy.

To test the efficacy of our model on such a diverse, recent and real-life dataset, we extracted documents from 4 top-level categories in Wikipedia in the following manner. \todo{rephrase}For each of the top-level category we compiled a list of all its child categories till a 3-level depth. Then, to create the document-category dataset, we considered all the documents and the categories assigned to them, that belonged to these categories. The four top-level categories we consider, hence, the datasets that we extract from Wikipedia are from the \emph{Physics}, \emph{Biology}, \emph{Mathematics} and \emph{Sports} categories. The number of documents, categories, words and other statistics of the extracted datasets are given in Table~\ref{wiki:data:stat}.

\begin{table}[h!]
%\tabcolsep=0.05cm
%\footnotesize
\begin{center}
\begin{tabular}{l c c c c c} % ccc ccc}
\toprule
& \textbf{$|\setD|$} & \textbf{$|\setC|$} & \textbf{$|\setW|$} & \textbf{Data Points} & \textbf{Sparsity}\\
\midrule
\textbf{Physics}		& 4\,229 & 2\,999 & 81\,614 & 14\,070 & 0.0010 \\
\textbf{Biology}		& 1\,604 & 2\,051 & 63\,767 & 5\,908 & 0.0018 \\
\textbf{Sports}			& 1\,529 & 2\,829 & 59\,058 & 3\,745 & 0.0008 \\
\textbf{Mathematics}	& 1\,193 & 1\,519 & 43\,398 & 3\,916 & 0.0013 \\
\bottomrule         
\end{tabular}
\caption{\label{wiki:data:stat}Statistics of the Wikipedia Datasets}
\end{center}
\end{table}

\section{Experimental Setup}
\label{sec:exp_setup}
In this section we describe in detail the data preprocessing steps, how we tune the hyper-parameters for both the document representation learning and the categorization algorithm, our evaluation criteria and techniques and also the baselines that we compare to.

\para{Data Preprocessing} : To curate the documents for learning their distributed representations we first split each document at sentence boundaries. We consider different sentences separately because we expect our system to learn syntactic qualities of the language which would not be possible if the sentence boundaries are ignored.
All independent numbers in the documents are converted to `$\backslash num$' and numbers that are a part of a word are left as it is. Eg. \emph{TH1RT3EN}, \emph{Se7en}. 
To remove noise in the documents we only consider those words that occur atleast $5$ times in the corpus.
We keep the capitalization of the words as it is because capitalization sometimes encodes a lot of semantic content that we do not wish to lose. Eg. \emph{Apple} in most cases is used to refer \emph{Apple Inc.} (Company) rather than the fruit. Similarly, \emph{apple} is most likely to refer to the fruit. Models that distinguish between the two forms will be able to learn better embeddings.

\para{Learning Document Embeddings} : To learn document representations using our model, we initialize the documents and word embeddings to small random vectors whose elements are drawn uniformly from the range $[\frac{-1}{k}, \frac{1}{k}]$. 
The value of the hyper-parameters of the model are chosen based on the performance on development data on the end-task of categorization. We could also find the optimum hyper-parameters by minimizing the development data perplexity on the document embedding learning task but as found in \citep{mnih2013learning}, lower perplexity does not ensure better representations but could also mean under-training. To choose the hyper-parameters we first use the default values as $k = 100$, $c = 2$, $n = 10$, \emph{number of epochs = 50}, $\gamma = 0.0025$, $\beta = 0.1$. We found that the learning rate $\gamma$ and the regularization constant $\beta$ give best performance across datasets at their default value. To choose the value of the other parameters, we first tune the embedding dimensions $k$ then the \emph{number of epochs} after which we consider different window sizes ($c$) and finally the number of negative samples $n$ for noise contrastive estimation. The values of the different hyper-parameters depend on the dataset. 

\para{Document Categorization} : For the task of document categorization, we split the document category data into training, development and test data by keeping $10\%$ documents for test purposes and $10\%$ for development. The rest $80\%$ are used for training purposes, i.e. learning category embeddings. We stop training based on the convergence reached on the development data. The value of the hyper-parameters learning rate $\gamma = 0.01$ and regularization constant $\beta = 0.01$ is chosen based on the performance on development data across different datasets. To make final predictions that whether the document should be assigned a particular category, we need to threshold the estimated probability, threshold for which was chosen based on the development data across datasets. It was found that the default logistic threshold of $0.5$ gave the best performance.

\para{Evaluation Criteria} : To evaluate the task of multi-label classification many evaluation criteria such as \emph{Hamming Loss}, \emph{Accuracy} and \emph{F1 score} have been used. We use the \emph{F1} score to evaluate our model i.e. the harmonic mean of the precision and recall values. We compute the F1 score in the micro-averaged fashion i.e. we combine prediction values for all the categories together for a particular dataset and then compute the precision and recall values. F1 score is a much more preferred measure compared to hamming loss or accuracy in the presence of imbalanced label distribution.

\para{Baselines} : To test the efficiency of our document representation learning algorithm, we compare our model's accuracy to some of the baseline method accuracies that are explained below.
\begin{enumerate}
\item \textbf{Bag-of-Words (BOW)} : 
The most famous document representation that has produced state-of-the-art results is the bag-of-words model with tf-idf weighting. We compare our document representations against the bag-of-words representations. We call this model the \textbf{BOW} model.

\item \textbf{Latent Semantic Indexing (LSI)} : 
As explained in Sec~\ref{sec:rw_dr}, Latent Semantic Indexing is a popular dimensionality reduction technique that uses the term co-occurrence statistics to capture the latent semantic structure of the documents. We use LSI to learn $100$-dimensional representation vectors for documents and compare our representations against them.
We call this the \textbf{LSI-100} model.

\item \textbf{Word Vector Averaging (WordVecAvg)} : 
With the availability of word vectors learned using the NPLM or the word2vec model, the most simple method to learn document embeddings is to take a weighted average of the word embeddings to represent the document. This method is similar to the bag-of-words technique as it ignores word ordering. We show that the representations learned using our method perform better than averaged vector representations. We call this the \textbf{WordVecAvg} model. The weighing scheme used to take the weighted average is \emph{tf-idf}.

\item \textbf{Probabilistic Matrix Factorization (PMF)} : 
The most simple technique for document categorization is the probabilistic matrix factorization of the document-category data matrix as explained in Sec.~\ref{sec:lr_similar_rl}. In this model, instead of learning only category embeddings, we also learn document representations simultaneously. Note that this model does not require any information about the document contents and also uses the document-category co-occurrence data. We call this the \textbf{PMF} model.

Also note that employing PMF for predicting categories from new documents is useless as they do not contain any category history in the training data and hence the document vectors would not be learned. PMF in such case would act as a trivial strawman. PMF can be used to predict categories for documents that already have category history in training data. 

\end{enumerate}
Apart from the above baselines, many more baselines for the \emph{Reuters-21578} dataset exist in literature that primarily use bag-of-words representation but use different learning algorithms. We also compare our model against them, details for which are given in the next section.

\section{Results}
\label{sec:results}
In this section we present our model's accuracy in categorizing new documents by learning distributed representations for the documents in the corpus and using historical data to learn category embeddings to predict categories for new documents. We compare our model's performance with baselines explained in the above section and also explain the process to choose hyper-parameters dependent on the dataset. 
We also present our model's performance in imputing categories for known documents in case the data has missing values. 

\subsection{Document Categorization}
As we see in \todo{some ref}, the task of categorizing documents with no prior categorization is of immense importance. In this section we present our model's efficacy in assigning categories to documents with no prior categorization. 

For evaluation of a particular dataset, we first divide it into training, development and test sets by keeping $80\%$ of the documents for training purposes and then dividing rest of the documents equally for development and testing. For each of the different hyper-parameter settings for the representation learning phase, we first learn document representations for all the documents. We then use the training document-category data to learn category embeddings and evaluate our model based on the development data. Using the accuracies obtained on the development data for different hyper-parameter settings we select the ``\emph{best}'' model (hyper-parameters, document and category-embeddings) which is then used to report results on the test data.

\subsubsection{Reuters - 21578}
The performance of our model on the development data for different values of different hyper-parameters is given in Table.~\ref{reuter:hp:k}, ~\ref{reuter:hp:epoch}, ~\ref{reuter:hp:c} and ~\ref{reuter:hp:n}. Table.~\ref{reuter:hp:k} and ~\ref{reuter:hp:epoch} shows that with increasing embedding dimensionality and the number of training epochs the accuracy of category prediction improves in terms of the F1 score. To avoid overfitting we use $k = 100$ and $epoch = 200$. In Table.~\ref{reuter:hp:c} we see that the default window size of $c=2$ performs best and Table~\ref{reuter:hp:n} shows that $10$ - $15$ negative samples per positive sample should be used for NCE.

\begin{table}[tb]
\tabcolsep=0.1cm
\footnotesize
\begin{center}
\begin{tabular}{l@{\hskip5mm} c c@{\hskip4mm} c@{\hskip5mm} c c@{\hskip4mm} c@{\hskip5mm} c c@{\hskip4mm} c}
\toprule
& \multicolumn{9}{c}{\textbf{Hyper-parameters} : {$epochs = 50$, $c = 2$, $n = 10$}}         \\
\cmidrule(lr){2-10}
\textbf{Tuning}
& \multicolumn{3}{c}{{$k = 50$}}         
& \multicolumn{3}{c}{{\highest{$k = 100$}}}        
& \multicolumn{3}{c}{{$k = 150$}}        	\\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
%\cmidrule(lr){2-10}
\multirow{2}{*}{\textbf{Reuters} (Development)}
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
& 88.7	 & 89.9	 & 89.3
& 90.9	 & 89.8	 & \highest{90.3}
& 90.9	 & 89.8	 & \highest{90.3} \\
\bottomrule         
\end{tabular}
%\vskip -4mm
\caption{\label{reuter:hp:k}\footnotesize {Performance on Reuters development data for different embedding dimensionality}}
\end{center}
\end{table}

\begin{table}[tb]
\tabcolsep=0.1cm
\footnotesize
\begin{center}
\begin{tabular}{l c c c c c c c c c c c c c c c}
\toprule
& \multicolumn{15}{c}{\textbf{Hyper-parameters} : {$k = 100$, $c = 2$, $n = 10$}}         \\
\cmidrule(lr){2-16}
\textbf{Tuning}
& \multicolumn{3}{c}{{$epochs = 50$}}         
& \multicolumn{3}{c}{{$epochs = 100$}}         
& \multicolumn{3}{c}{{$epochs = 150$}}         
& \multicolumn{3}{c}{{$epochs = 200$}}         
& \multicolumn{3}{c}{{$epochs = 250$}}	\\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\cmidrule(lr){11-13}
\cmidrule(lr){14-16}
\multirow{2}{*}{\textbf{Reuters} (Development)}
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\cmidrule(lr){11-13}
\cmidrule(lr){14-16}
& 90.9	 & 89.8	 & 90.3
& 92.3   & 92.9  & 92.6
& 93.3   & 93.0  & 93.2
& 93.7   & 93.9  & \highest{93.8}
& 94.0   & 93.1  & 93.6 \\
\bottomrule         
\end{tabular}
\caption{\label{reuter:hp:epoch}Performance on Reuters development data for different number of epochs}
%\vskip -4mm
\end{center}
\end{table}

\begin{table}[h!]
\tabcolsep=0.1cm
\footnotesize
\begin{center}
\begin{tabular}{l@{\hskip5mm} c c@{\hskip4mm} c@{\hskip5mm} c c@{\hskip4mm} c@{\hskip5mm} c c@{\hskip4mm} c}
\toprule
& \multicolumn{9}{c}{\textbf{Hyper-parameters} : {$k = 100$, $epochs = 200$, $n = 10$}}         \\
\cmidrule(lr){2-10}
\textbf{Tuning}
& \multicolumn{3}{c}{{$c = 2$}}         
& \multicolumn{3}{c}{{$c = 3$}}        
& \multicolumn{3}{c}{{$c = 4$}}        	\\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
%\cmidrule(lr){2-10}
\multirow{2}{*}{\textbf{Reuters} (Development)}
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
& 93.7   & 93.9  & \highest{93.8}
& 93.2   & 90.9  & 92.0
& 91.8   & 90.8  & 91.3 \\
\bottomrule         
\end{tabular}
%\vskip -4mm
\caption{\label{reuter:hp:c}\footnotesize {Performance on Reuters development data for different window sizes}}
\end{center}
\end{table}

\begin{table}[h!]
\tabcolsep=0.1cm
\footnotesize
\begin{center}
\begin{tabular}{l@{\hskip5mm} c c@{\hskip4mm} c@{\hskip5mm} c c@{\hskip4mm} c@{\hskip5mm} c c@{\hskip4mm} c}
\toprule
& \multicolumn{9}{c}{\textbf{Hyper-parameters} : {$k = 100$, $epochs = 200$, $c = 2$}}         \\
\cmidrule(lr){2-10}
\textbf{Tuning}
& \multicolumn{3}{c}{{$n = 5$}}         
& \multicolumn{3}{c}{{$n = 10$}}        
& \multicolumn{3}{c}{{$n = 15$}}        	\\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
%\cmidrule(lr){2-10}
\multirow{2}{*}{\textbf{Reuters} (Development)}
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} 
& {P} & {R} & \textbf{F1} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
& 92.5   & 89.5  & 91.0
& 93.7   & 93.9  & \highest{93.8}
& 94.7   & 92.4  & 93.5 \\
\bottomrule         
\end{tabular}
%\vskip -4mm
\caption{\label{reuter:hp:n}\footnotesize {Performance on Reuters development data for different number of negative samples}}
\end{center}
\end{table}

Table~\ref{reuter:cs} compares our model's accuracy against different document representations and other learning algorithms that use bag-of-words representations. 
We find that document representations learned using our model performs the best. Our representations improves the F1 score of $84.1\%$, achieved when using bag-of-words representation, by $9\%$. 
Improvements of $1\%$-$6.6\%$ on the F1 score are found against models such as SVM, CRF, LSI, MFoM and representations learned using tf-idf weighted average of word vectors.
We also see that simple summing of word vectors to compute the projection layer reduces the performance of our model and hence leads to poor quality document representations. This corroborates with our hypothesis that weighted average of surrounding words is necessary to capture syntactic qualities and learn better quality representations.

\begin{table}[h!]
\tabcolsep=0.1cm
\footnotesize
\begin{center}
\begin{tabular}{l@{\hskip5mm} c c@{\hskip4mm} c}
\toprule
% & \multicolumn{3}{c}{Reuters-21578}         \\
% \cmidrule(lr){2-4}
\textbf{Reuters-21578} & {P} & {R} & \textbf{F1} \\
\midrule
\textbf{BOW}
& 77.8   & 91.5  & 84.1 \\
\textbf{LSI-100}
& 84.8   & 96.7  & 90.4 \\
\textbf{WordVecAvg}
& 94.1   & 88.1  & 91.0 \\ \addlinespace[1mm]

\textbf{CMLF (CRF)}
& -   & -  & 87.0 \\
\textbf{SVM (poly)}
& -   & -  & 86.0 \\
\textbf{SVM (rbf)}
& -   & -  & 86.4 \\ 
\textbf{Binary-MFoM}
& -   & -  & 88.4 \\ 
\textbf{MC-MFoM}
& -   & -  & 88.8 \\ 

\addlinespace[1mm]
\textbf{Our Model}
& \multirow{2}{*}{92.1}   & \multirow{2}{*}{86.1}  & \multirow{2}{*}{89.0} \\
(no weight) & & & \\ \addlinespace[1mm]
\textbf{Our Model}
& \multirow{2}{*}{94.1}   & \multirow{2}{*}{89.3}  & \multirow{2}{*}{\highest{91.7}} \\
(with weights) & & & \\
\bottomrule         
\end{tabular}
%\vskip -4mm
\caption{\label{reuter:cs}\footnotesize {\textbf{Document Categorization on Reuters-21578(\emph{ModApte}):} Precision/Recall/F1 on document categorization of our model compared against different document representations and complex learning algorithms on the Reuters-21578 dataset.}}
\end{center}
\end{table}

\begin{figure}[tb]
\centering
        \includegraphics[width=\columnwidth]{figs/pr/reuter-cs.pdf}
    \caption{Precision/Recall for Document Categorization on Reuters-21578}
    \label{fig:pr:reuter:cs}
\end{figure}


\subsubsection{Physics - Wikipedia}

