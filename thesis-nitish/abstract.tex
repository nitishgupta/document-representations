\begin{center}
\huge{\textbf{Abstract}}
\end{center}

Multi-label Document Categorization, the task of automatically assigning a text document into one or more categories is a crucial step in knowledge management and has various real-world applications such as categorizing news articles, tagging Web pages, maintaining medical patient records and organizing digital libraries among many others. 
Statistical Machine Learning approaches to document categorization have focused on multi-label learning algorithms such as Support Vector Machines, k-Nearest Neighbors, Logistic Regression, Neural Networks, Naive Bayes, Generative Probabilistic Models etc. while the input to such algorithms i.e. the vector representation for documents has traditionally been used as the bag-of-words representation model. 
%The input to such algorithms i.e. the vector representation for documents has traditionally been used as the bag-of-words representation model due to its simplicity and ability to capture the topical content of the documents.
Though the usage of simple bag-of-words document representation gives surprisingly accurate results, it suffers from sparsity, high-dimensionality and lack of similarity measures along with various drawbacks such as the inability to preserve word ordering and contextual information in which the words occur in the documents. Encoding contextual information about words in documents is crucial to capture the correct semantic content of the highly complex and ambiguous human language.  \hfill \break

Our work is focused on learning continuous distributed vector representations for documents by embedding all the documents in the same low-dimensional space such that documents that are similar in their semantic content have similar vector representations. To tackle the issues in bag-of-words representation model, we present an unsupervised neural network model that, given a word in a document, uses the document representation along with the contextual information in which the word occurs, to predict it and learn document representations along with learning distributed word vectors. 
%By learning low-rank distributed representations and incorporating contextual information in which the words occur in the documents we try to overcome the drawbacks posed by the bag-of-words representations. 
We use a modified version of the logistic regression algorithm to learn similar distributed representations for categories to perform the document categorization task. As we embed documents, categories and words in the same low-dimensional space, estimating similarity between them is as simple as taking a dot-product between the vectors of the entities. We show that the representations learned using our model give state-of-the-art results in the document categorization task on standard \emph{Reuters-21578} and Wikipedia datasets and also show promising results in imputing missing categories in existing articles on Wikipedia against the bag-of-words representations.