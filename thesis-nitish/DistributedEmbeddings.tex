\chapter{Distributed Document Representations}
\label{chapter:distembed}
In this chapter we describe the concept of distributed word and document representations and why distributed representations of words and documents are better than one-hot or bag-of-words representations as described in \ref{sec:textrepr}. We then discuss the different models for learning distributed word representations in a fully unsupervised manner. Finally, we describe in detail our proposed model for learning distributed document representations.

\section{Need for Distributed Word Representations}
\label{sec:motivation_distributed}
Words are regarded as atomic symbols in most rule-based and statistical natural language processing (NLP) tasks and hence need appropriate representation to solve the NLP tasks with greater ease and accuracy. 
Words are traditionally expressed as one-hot vectors, i.e. as vectors of the size of the vocabulary where exactly one element is $1$ and the rest all are zero.
Though these representations have been widely used, one-hot representations have a plethora of drawbacks that pose problems and limit the ability of systems to perform better. 
\begin{enumerate}
\item \textbf{Curse of Dimensionality} : One-hot representations have word vectors that are the size of the vocabulary which often consists of tens to hundreds of thousands of words. Due to this curse of dimensionality, language modeling becomes almost impossible where the number of parameters can grow exponentially with the size of the vocabulary.

\item \textbf{No Word Similarity} : As words are represented by sparse 
orthogonal vectors, there is no notion of word similarity that can be 
introduced. In one-hot representation, the word ``symphony'' is 
equally close to the words ``bark'' and ``guitar''. We would want word 
representations such that they capture the semantic or topical 
similarity between words.
\end{enumerate}

Due to the problems discussed above there is a need for continuous, 
low-dimensional, non-sparse vector representations for words that capture 
their semantic content and can be used to model language with 
continuous distributions and can be used as inputs for various other 
NLP tasks. 

\section{Background on Word Embeddings}
\label{sec:background_distributed}
Distributed word representations are dense, fixed-sized feature vectors 
learned for words in an unsupervised manner from a large text corpus 
that capture the semantic content of words. Each word $w_{i}$ in the 
corpus is represented by a vector, $v_{w_{i}} \in \mathbb{R}^{m}$, 
where $m$ usually ranges from $50-300$ dimensions. These dense 
representations help address sparsity and high-dimensionality problems 
of one-hot representations and also allow estimation of
similarities between words; which is as simple as taking the dot-product 
or calculating the cosine-distance between the vectors. 

All of the word vector learning models make use of neural networks 
\citep{bengio2003neural, mnih2013learning, mikolov2013distributed, collobert2011natural, bottou2014machine, turian2010word, levy2014dependencybased} but differ in their learning objectives. 

Below we describe a few models to show how different learning objectives and architecture can lead to learning high-quality word vectors with different properties.

\subsection{Neural Probabilistic Language Model}
\label{sec:bengio}
Neural Probabilistic Language Model (NPLM), introduced by 
\citet{bengio2003neural}, aims to learn distributed word vectors and a 
probability function that uses these vectors to learn a statistical 
model of language. In their model, the probability of a word sequence 
is expressed as the product of conditional probabilities of the next 
word given the previous ones. 
\begin{equation}
P(w_{1}^{T}) =  \prod_{t=1}^{T} P(w_{t}| w_{1}^{t-1})
\end{equation}
And making the n-gram assumption, 
\begin{equation}
P(w_{t} | w_{1}^{t-1}) \approx P(w_{t} | w_{t-n+1}^{t-1})
\end{equation}
i.e. the probability of the next word in the sequence is mostly affected by the local context, in this the previous $n$-words and not the whole past sequence.

Their model maps each word to an $m$-dimensional vector in a matrix $C \in \mathbb{R}^{|V|\times m}$ and estimates the probability $P(w_{t} = i|w_{t-n+1}^{t-1})$ i.e. the probability that the $t^{th}$ word in the sequence is $w_{i}$. The neural network that is used to estimate this probability using the word vectors is shown in Figure~\ref{fig:nn:bengio}.
\begin{figure}[t!]
    \centering
        \includegraphics[width=0.8\textwidth]{figs/bengio_nn.png}
    \caption{Neural Network Architecture for Neural Probabilistic Language Model (NPLM). \cite{bengio2003neural}}
    \label{fig:nn:bengio}
\end{figure}
For each input sequence, the neural network outputs a vector $y \in \mathbb{R}^{|V|}$, where $y_{i}$ is the unnormalized log-probability that the $t^{th}$ word in the sequence is $w_{i}$. 
The output $y$ is computed as,
\begin{equation}
y = b + Wx + Utanh(d + Hx)
\end{equation}
where $tanh$ is the hyperbolic tangent applied to introduce non-linearity and $x$ is the word feature layer activation vector constructed by the concatenation of the context word vectors,
\begin{equation}
x = (C(w_{t-1}), C(w_{t-2}), \ldots, C(w_{t-n+1}))
\end{equation}
The unnormalized log probabilities in $y$ are converted to positive probabilities summing to $1$ by using a \emph{softmax} output layer that computes, 
\begin{equation}
P(w_{t} = i | w_{t-1}, \ldots, w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}e^{y_{i}}}
\end{equation}
The parameters of the model $(b, d, W, U, H)$ and the word vectors $C$ are estimated by maximizing the log-likelihood of the training corpus.

\subsection{Log-Linear Models}
\label{sec:word2vec}
Simple log-linear models are proposed in \citet{mikolov2013efficient} as opposed to the non-linear NPLM model to bring down the training time complexity without sacrificing the quality of the word vectors. It achieves this by not having a non-linear layer and matrix weighting of the input vectors, that are the costliest operations in NPLM. The two models proposed in \citet{mikolov2013efficient} (word2vec) are Continuous Bag-of-Words model and Continuous Skip-Gram model, described below.

\subsubsection{Continuous Bag-of-Words Model}
\label{sec:cbow}
The Continuous Bag-of-Words (CBOW) model is different from the NPLM in that the projection layer is shared for all words; i.e. all words get projected into the same hidden layer vector (their vectors are averaged). This architecture neglects the ordering of the words as opposed to NPLM that uses the concatenation of input vectors for the projection layer. The training criteria in this model is to predict the current (middle) word given its context. It also uses word sequences from the future to aid this task with the relaxation that the aim is not to learn a language model. The model architecture is given in Figure~\ref{fig:nn:cbow}.
\begin{figure}[h!]
    \centering
        \includegraphics[width=0.5\textwidth]{figs/mikolov_cbow.png}
    \caption{Continuous Bag-of-Words Model (CBOW). Ref. \cite{mikolov2013efficient} }
    \label{fig:nn:cbow}
\end{figure}
The model first computes the hidden layer vector $h$, 
\begin{equation}
h(w_{t-k}, \ldots, w_{t+k}) = \frac{w_{t-k} + \ldots + w_{t-1} + w_{t+1} + \dots + w_{t+k}}{2k}
\end{equation}
where, $w_{t-i}$ is the $i$-th previous word in the context of the middle word $w_{t}$ and $k$ is the window length.
The neural network then computes an unnormalized log-probability vector $y$ similar to Sec.\ref{sec:bengio}, and uses the \emph{softmax}-classifier to estimate $P(w_{t}|w_{t-k}, \ldots, w_{t+k})$,
\begin{equation}
y = b + Uh(w_{t-k}, \ldots, w_{t+k})\\
\end{equation}
\begin{equation}
\label{eq:cbow:prob}
P(w_{t}|w_{t-k}, \ldots, w_{t+k}) = \frac{e^{y_{w_t}}}{\sum_{i} e^{y_{i}}}
\end{equation}
The parameters of the CBOW model, $(b, U)$ and the word vectors ($w_{i}$) are learned by maximizing the average log probability (Eq.~\ref{eq:cbow:prob}) of the training corpus.

\subsubsection{Continuous Skip-gram Model}
This model is similar to the CBOW model, but instead of predicting the middle word based on the context, it tries to maximize the classification of a word based on another word in the context. More precisely, given each word, the skip-gram model tries to predict words within a certain range before and after the current word. The model architecture is given in Figure~\ref{fig:nn:skip}.
\begin{figure}[h!]
    \centering
        \includegraphics[width=0.4\textwidth]{figs/mikolov_skip.png}
    \caption{Continuous Skip-gram Model. Ref. \cite{mikolov2013efficient}}
    \label{fig:nn:skip}
\end{figure}
Formally, given a sequence of words in a context $w_{t-k}, \ldots, w_{t+k}$, the skip-gram model defines $P(w_{t+j}|w_{t})$ using the \emph{softmax}-classifier in the following manner,
\begin{equation}
\label{eq:skip:prob}
P(w_{t+j}|w_{t}) = \frac{e^{(v_{w_{t}} \cdot v_{w_{t+j}} )} }{\sum_{i} e^{(v_{w_{t}} \cdot v_{w_{i}})} }
\end{equation}
The only parameters of the Skip-gram model are the word vectors ($v_{w_{i}}$) that are learned by maximizing the average log probability (Eq.~\ref{eq:skip:prob}) of predicting all the context words for all the words in the training corpus.

The CBOW and the Skip-gram models use the \emph{hierarchical softmax} \citep{morin2005hierarchical} instead of the full softmax to speed-up the learning process.

The quality of the word vectors is tested using the \emph{Semantic-Syntactic Word Relationship test} that evaluates the model performance on retrieving semantically and syntactically similar words to the given test words. The word vectors learned using the skip-gram model are also shown to encode many linear linguistic regularities and patterns \citep{mikolov2013linguistic} and show additive compositionality using simple vector arithmetic. For example, the result of the vector calculation $vec(Madrid) - vec(Spain) + vec(France)$ is closest to $vec(Paris)$ than any other word vectors.

\subsubsection{Dependency-based Word Embeddings}
Instead of using a bag-of-words based context as used in \emph{NPLM} 
and \emph{word2vec}, \citet{levy2014dependencybased} use arbitrary 
contexts to investigate its effects on the word vectors and the 
properties they encode. The most important of their techniques is to 
derive the contexts based on the syntactic relations that the word 
participates in. For each word $w$ and its modifiers $m_1, \ldots, m_k$ 
found using the parse tree of the sentence, contexts 
$(m_{1}, lbl_{1}, \ldots, m_{k}, lbl_{k})$ are extracted, where $lbl$ 
is the type of the dependency relation between word and the modifier 
and $lbl^{-1}$ is used to mark the inverse-relation. An example of the 
contexts extracted for a sentence is given in Figure~\ref{fig:dep:context}.
\begin{figure}[h!]
    \centering
        \includegraphics[width=0.7\textwidth]{figs/dependency_context.png}
    \caption{Example for Dependency-based context extraction from \cite{levy2014dependencybased}}
    \label{fig:dep:context}
\end{figure}
After context extraction, their model uses the neural network architecture and the learning objective of the skip-gram model to learn word vectors. On comparison to the vectors learned from the skip-gram model on the tasks of \emph{topical similarity} and \emph{functional similarity} estimation, it is found that the vectors learned from this model perform better on the \emph{functional similarity} task that expects word vectors to encode syntactic relationships better. The task of \emph{topical similarity} estimation showed that vectors from the skip-gram model encode semantic content better because of the bag-of-words context used during training.

\section{Document Representation}
\label{sec:document_embeddings}
Though the semantic word spaces as described above are very useful for a lot of NLP tasks, their ability to capture the complexity and compositionality of human language is limited. 
Word embeddings cannot be directly used to represent longer phrases, sentences and documents to express their meaning. 
Tasks such as word sense disambiguation, sentiment analysis, text categorization etc. all require the text representation to capture the semantic content of the text for better inputs to learning algorithms as compared to a simple bag-of-words model. 

Progress towards learning distributed representations for longer pieces of text, such as phrase-level or sentence-level representations \cite{mitchell2010composition, zanzotto2010estimating, yessenalina2011compositional, grefenstette2013multi, mikolov2013distributed} that capture semantic compositionality has been promising, but most models do not go beyond simple weighted average of word vectors to represent longer texts. 
\citet{socher2013recursive} proposes a more sophisticated approach using recursive tensor neural network where the dependency parse-tree of the sentence is used to compose word vectors in a bottom-up approach to represent sentences for sentiment classification of phrases and sentences. 
Both the techniques have weaknesses for learning document representations. The first approach is analogous to a bag-of-words approach and neglects word order while representing documents whereas the second approach considers syntactic dependencies but cannot go beyond sentences as it relies on parsing.

%\subsection{Learning Document Embeddings : Our Approach}
Below we present our model on learning universal distributed representations for documents and words in the corpus such that,
\begin{enumerate}
\item The learned document representations encode different semantic and topical content of the documents.
\item Documents and words are embedded in the same $k$-dimensional space such that semantically similar entities have similar vector representations.
\end{enumerate}
%Our model is inspired by the work on continuous bag-of-words model \citep{mikolov2013efficient} and \citep{le2014distributed} model of learning representations for sentences and paragraphs. 
To learn vectors that satisfy $1.$ and $2.$, we hypothesize that document representations should be learned such that they can aid in the prediction of words in a given word sequence from the document. In the sections below we formally introduce the problem and present our model to learn document and word vector representations.

\subsection{Problem Setup}
Given a set of documents, $\setD=\{d_{1}, \ldots, d_{|\setD|}\}$ and a vocabulary of words, $\setW$ constructed using the given documents, we wish to embed each document $d_{i} \in$ \setD and each word in the vocabulary onto the same $k$-dimensional space such that the learned vectors encode semantic content of the entities. 

For every sequence of words $w_{t-c}, \ldots, w_{t+c}$ in, say document $d_{i}$, we wish to estimate the probability $p(w_{t}|d_{i}, w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})$ of predicting the middle word in the sequence using the information about the document and the words in the context. 
We will estimate this probability using the vector representations for documents and words and learn vectors such that the probability of predicting the middle word correctly in the context is maximized.

\subsection{Our Model}
\label{sec:docem_ourmodel}
A document $d_{i} \in \setD$, indexed by `$i$', in our model is represented by a vector $\vecdi{i} \in \mathbb{R}^{k}$, which is also the $i$-th column of the matrix $\matD = \left[\vecdi{1}, \ldots, \vecdi{|\setD|}\right] \in \mathbb{R}^{k \times |\setD|}$. 
Similarly, a word indexed by `$i$' in the vocabulary $\setW$ is represented by the vector $\vecwi{i} \in \mathbb{R}^{k}$, which is also the $i$-th column of the matrix $\matW = \left[\vecwi{1}, \ldots, \vecwi{|\setW|}\right] \in \mathbb{R}^{k \times |\setW|}$.

Given a sequence $(w_{t-c}, \ldots, w_{t+c})$ of $2c+1$ words and the document it occurs in, our training objective is to maximize the probability of correctly predicting the middle word, $p(w_{t} | d_{i}, w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})$, using the surrounding context words and the information about the document in terms of their distributed vector representations. 
%Therefore, our training objective is to maximize the probability $p(w_{t} | d_{i}, w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c})$ of correctly predicting the middle word using the information about the surrounding words and the document the sequence occurs in. 

To learn the distributed document and word representations, we present a neural network model using which we,
\begin{enumerate}
\item Represent each document and word in the corpus by a $k$-dimensional vector stored in the matrices $\matD$ and $\matW$, respectively.
\item Estimate the probability of predicting the middle word in a given word sequence using the vector representation of the the words in the context and the document it occurs in.
\item Learn the word and document vectors along with the parameters of the model simultaneously to estimate the probability.
\end{enumerate}
The architecture for the proposed neural network is given in Fig.~\ref{fig:nn:archi}.
Also note that the word vector representations learned and stored in the matrix $\matW$ are universal representations and shared across all documents and contexts.
\begin{figure}[h!]
    \centering
        %\includegraphics[width=0.9\textwidth]{figs/nn_arch.png}
        \input{figs/nn_arch}
    \caption{Neural network Architecture for our model}
    \label{fig:nn:archi}
\end{figure}

%\para{Projection Layer (Context Representation)} : 
\subsubsection{Context Representation}
We represent the context(words surrounding the middle word to be predicted) and the document together in the same projection layer, denoted by $h_{c} \in \mathbb{R}^{k}$, by taking a weighted sum of the corresponding vector representations. 
The weights for the context words $\Lambda = \{\wgt{i} | i=\{t-c, \ldots, t-1, t+1, \ldots, t+c\}\}$ are kept universal for different sequences across the corpus as we expect the weights to learn some kind of syntactic aspect of the language to better represent the context. Also the weight corresponding to the document vector is kept constant at $1$. 
We also (unsuccessfully) experimented by taking matrix weights instead of scalar weights($\wgt{i}$) to learn better syntactic aspects of the language. 
The projection layer representation is computed using,
\begin{equation}
\label{eq:hidden_vec}
h_{c} = \vecdi{d_{i}} + \wgt{t-c}\vecwi{w_{t-c}} + \ldots + \wgt{t-1}\vecwi{w_{t-1}} + \wgt{t+1}\vecwi{w_{t+1}} + \ldots + \wgt{t+c}\vecwi{w_{t+c}}
\end{equation}

%\textbf{Probability Prediction} : 
\subsubsection{Estimating Prediction Probability}
In absence of any non-linearity we expect that the projection layer vector should be aligned to the correct middle word of the sequence. Hence we estimate the probability of predicting the word $w_{t}$ as the middle word in the following manner. 
\begin{enumerate}
\item An output score $s_{w_{i}} \in \mathbb{R}$ for every $w_{i}$ in the vocabulary is estimated by,
\begin{equation}
\label{eq:nn_score}
s_{w_{i}} = \sigma(\vecwi{w_{i}}\cdot h_{c})
\end{equation}
where $\sigma(z) = \frac{1}{1+e^{-z}}$ is the standard sigmoid function. 
\item After calculating the score for each word in the vocabulary, we use the \emph{softmax} classifier to estimate the probability of predicting the actual correct word $w_{t}$ as the middle word in the sequence,
\begin{equation}
\label{eq:soft_prob}
p(w_{t} | d_{i}, w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c}) = \frac{e^{s_{w_{t}}}}{\sum_{i \in \setW} e^{s_{w_{i}}}}
\end{equation}
\end{enumerate}

%\textbf{Training Objective} : 
\subsubsection{Learning Objective}
The training data $\traindata$, is composed of $M$ training sequences each of which is a $2c+1$ length sequence of words and the document index it belongs to. For example, $t = \{d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}\}$ represents the $m^{th}$ training sequence in $\traindata$.  

Given the training data $\traindata$, our objective is to learn an optimum parameter set $\Theta = (\matD, \matW, \Lambda)$ consisting of the document and word vector matrices and the projection layer weights for the context words, by maximizing the average log probability of estimating the middle word correctly in all the training word sequences where the probability of estimating the middle word as $w_{i}$ is given by Eq.~\ref{eq:soft_prob}. Therefore, 
\begin{equation}
\label{eq:paramter_argmax}
\hat{\Theta} =  \argmax_{\Theta}~l(\traindata, \Theta)
\end{equation}
\begin{equation}
\label{eq:training_objective}
l(\traindata, \Theta) = \frac{1}{M}\sum_{m=1}^{M} \log \left[p(w^{(m)}_{t} | d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t-1}, w^{(m)}_{t+1}, \ldots, w^{(m)}_{t+c})\right]
\end{equation}
To optimize the training objective in Eq.~\ref{eq:training_objective}, we can use the Stochastic Gradient Descent(SGD) algorithm to find the gradient of the objective function (Eq.~\ref{eq:training_objective}) w.r.t. to the individual parameters $\theta_{i}$ and apply the update rule as follows,
\begin{equation}
\label{eq:update_theta}
\theta^{(x)}_{i} = \theta^{(x-1)}_{i} + \gamma\frac{\partial l(\traindata, \Theta)}{\partial \theta_{i}}
\end{equation}
where $x$ is the current iteration number and $\gamma$ is the learning rate. Note that we add the gradient to $\theta^{(x-1)}_{i}$ because we wish to maximize the training objective.
Updating the parameters for sufficient number of iterations would yield the optimum document and word vectors along with the weights for the neural network.

\subsubsection{Noise Contrastive Estimation}
As we see in Eq~\ref{eq:soft_prob}, estimating the probability for each training word sequence requires a sweep through the whole vocabulary of size $|\setW|$ which can be a very expensive computation given that typical vocabulary sizes range from a few tens to a few hundreds of thousand words for large datasets. Approaches to reduce this training time, such as, use of hierarchical soft-max \citep{morin2005hierarchical} and use of importance sampling to approximate the likelihood gradient \citep{bengio2003quick, bengio2008adaptive} have been proposed. 
Hierarchical softmax reduces the training time from linear to logarithmic in vocabulary size but is considerably more involved and finding well-performing trees is not trivial. Also, though importance sampling provides substantial speedups, it suffers from stability problems.

Noise Contrastive Estimation (NCE) \citep{gutmann2012noise} is a method for fitting unnormalized probabilities by reducing the problem of \emph{probability density estimation} to \emph{probabilistic binary classification}. It has also been adapted to NPLM (Sec.~\ref{sec:bengio}) \citep{mnih2012fast} and learning word embeddings. \citet{mnih2013learning} shows significant improvements in training time without too much degradation in the quality of word vectors learnt.

The basic idea of NCE as incorporated in our model is to learn a logistic classifier to distinguish between the correct middle word in the given word sequence and corrupt samples from some ``noise'' distribution. Therefore, given a training sequence of the form $t = \{d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}\}$, our training objective now is to train a classifier such that it can distinguish between a positive training sample $w^{(m)}_{t}$ as a positive example and negative training samples $w_{x}$ drawn from a noise distribution $P_{n}(w)$ as negative examples for the middle word given the surrounding words (context) and the document the sequence belongs to.

Our training data $\traindata$ is now converted to a set of labeled sequences of the form 
%$$t = \left[ \{d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}\}, Y^{(m)}=1 \right]^{m=M}_{m=1}$ 
$ \{d^{(m)}_{i}, w^{(m)}_{t-c}, \ldots, w^{(m)}_{t+c}, Y^{(m)}=1\}^{m=M}_{m=1} $
where $Y=1$ denotes that the sequence is a positive sample occurring in the corpus. 
For every positive training sequence $t$ we also have $n$ corrupt training sequences where, in each of them only the middle word $w_{t}$ has been replaced by a corrupt word sampled from the noise distribution $P_{n}(w)$ and the value of the label $Y=0$. 
Therefore, for every positive training example there exists $n$ negative training examples and the total number of training samples in $\traindata$ is $M + nM$ now. 
We now need to train a binary classifier such that, given a sequence of words and the document it belongs to, it can predict correctly whether the sequence is legitimate (correct value of the label indicator $Y$). 

Given a training sequence we estimate the probability that the given sequence is positive using,
\begin{equation}
\label{eq:label1}
P(Y=1|d_{i}, w_{t-c}, \ldots, w_{t+c}, \Theta) = \sigma(\vecwi{w_{t}} \cdot h_{c})
\end{equation}
where $h_{c}$ is the projection layer vector calculated using Eq.~\ref{eq:hidden_vec}. Similarly, the probability of estimating that a given sequence is corrupt is given by,
\begin{equation}
\label{eq:label0}
P(Y=0|d_{i}, w_{t-c}, \ldots, w_{t+c}, \Theta) = 1 - \sigma(\vecwi{w_{t}} \cdot h_{c})
\end{equation}
From Eq.~\ref{eq:label1} and Eq.~\ref{eq:label0} we get,
%Therefore, the probability of $Y$ taking the value $0$ or $1$ is,
\begin{equation}
\label{eq:prob_y}
P(Y|d_{i}, w_{t-c}, \ldots, w_{t+c}, \Theta) = [\sigma(\vecwi{w_{t}} \cdot h_{c})]^{Y}[1 - \sigma(\vecwi{w_{t}} \cdot h_{c})]^{1-Y}
\end{equation}
As a shorthand notation, we would express the probability estimation in Eq.~\ref{eq:prob_y} as $P_{\Theta}(Y)$.

\subsubsection{Learning Objective using NCE}
Our new training objective involves maximizing the log-likelihood of observing the modified training data $\traindata$ that includes the negative examples sampled from the noise distribution $P_{n}(w)$ along with the original positive training sequences. 	
\begin{equation}
\label{eq:new_argmax}
\hat{\Theta} =  \argmax_{\Theta}~l(\traindata, \Theta)
\end{equation}
\begin{equation}
\label{eq:new_training_objective}
l(\traindata, \Theta) = \sum_{m=1}^{M + nM} \log P_{\Theta}(Y_{m} = Y^{(m)})
\end{equation}
where $Y_{m}$ is the predicted label for the $m$-th training sequence and,
\begin{equation}
\label{eq:log_P}
\log P_{\Theta}(Y_{m} = Y^{(m)}) = Y^{(m)}\log \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c}) + (1 - Y^{(m)})\log (1 - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c}))
\end{equation}

\subsubsection{Parameter Estimation}
\label{sec:para_esti_doc}
To learn the optimum parameters $\Theta = (\matD, \matW, \Lambda)$, we maximize the log-likelihood of observing the training data given in Eq~\ref{eq:new_argmax} using the Stochastic Gradient Descent(SGD) algorithm described below. 

Firstly, for the SGD algorithm we need to calculate the gradient of the log probability estimate (Eq~\ref{eq:log_P}) with respect to individual parameters $\theta \in \Theta$. The derivative of the log probability estimate w.r.t. to a parameter $\theta \in \Theta$ is given by,
\begin{align}
\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \theta} &= \left[ Y^{(m)}\frac{1}{\sigma(d^{(m)})} - (1-Y^{(m)})\frac{1}{(1 - \sigma(d^{(m)}))}\right] \frac{\partial \sigma(d^{(m)})}{\partial \theta} \\
&= \left[ Y^{(m)}\frac{1}{ \sigma(d^{(m)})} - (1 - Y^{(m)})\frac{1}{(1 - \sigma(d^{(m)}))}\right]\times\notag\\ &\qquad\left[\sigma(d^{(m)})(1-\sigma(d^{(m)}))\right]\frac{\partial d^{(m)}}{\partial \theta} \\
\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \theta} &= \left[ Y^{(m)} - \sigma(d^{(m)})\right] \frac{\partial d^{(m)}}{\partial \theta}
\end{align}
where $d^{(m)} = (\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})$ is the dot-product of the projection layer vector with the word vector for the middle word. Therefore,
\begin{equation}
\label{eq:partial_theta}
\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \theta} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] \frac{\partial (\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})}{\partial \theta}
\end{equation}
For any training sequence $m$, there are four types of parameters $\theta$ that need to be updated. Firstly, the document vector for $d^{(m)}_{i}$, the middle word vector for word $w^{(m)}_{t}$, word vectors for context words $w^{(m)}_{t+j}$ and the neural network weights $\wgt{i}$. The derivate $\frac{\partial (\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})}{\partial \theta}$ w.r.t. each of them is given by,
\begin{equation}
\label{eq:partial_doc}
\frac{\partial (\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})}{\partial \vecdi{d^{(m)}_{i}}} = \vecwi{w^{(m)}_{t}}
\end{equation}
\begin{equation}
\label{eq:partial_w_t}
\frac{\partial (\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})}{\partial \vecwi{w^{(m)}_{t}}} = h^{(m)}_{c}
\end{equation}
\begin{equation}
\label{eq:partial_w_t-k}
\frac{\partial (\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})}{\partial \vecwi{w^{(m)}_{t+j}}} = \wgt{t+j}* \vecwi{w^{(m)}_{t}}
\end{equation}
\begin{equation}
\label{eq:partial_wgt_t-k}
\frac{\partial (\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})}{\partial \wgt{t+j}} = \vecwi{w^{(m)}_{t}} \cdot \vecwi{w^{(m)}_{t+j}}
\end{equation}

Therefore the derivative of the log-probability estimate w.r.t. the 
\begin{enumerate}
\item
Document Vector is given by, 
%\textbf{Document Vector} : 
\begin{equation}
\label{eq:grad_doc}
\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \vecdi{d^{(m)}_{i}}} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] \vecwi{w^{(m)}_{t}}
\end{equation}
\item 
%\textbf{Middle Word} : 
Middle word is given by,
\begin{equation}
\label{eq:grad_mword}
\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \vecwi{w^{(m)}_{t}}} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] h^{(m)}_{c}
\end{equation}
\item 
%textbf{Context Word} : 
Context words is given by,
\begin{equation}
\label{eq:grad_cword}
\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \vecwi{w^{(m)}_{t+j}}} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] \wgt{t+j} \vecwi{w^{(m)}_{t}}
\end{equation}
\item 
Neural Network weights is given by
%\textbf{Neural Network Weight} : 
\begin{equation}
\label{eq:grad_wgt}
\frac{\partial \log P_{\Theta}(Y_{m}=Y^{(m)})} {\partial \wgt{t+j}} = \left[ Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})\right] (\vecwi{w^{(m)}_{t}} \cdot \vecwi{w^{(m)}_{t+j}})
\end{equation}
\end{enumerate}
According to the SGD algorithm, the update to be made to a parameter $\theta \in \Theta$ on observing a training sequence $m$ is therefore given in Eq.~\ref{eq:update_reg}. We also include $L_{2}$ regularization for the parameters as it helps in avoiding overfitting and restricts the  parameters from attaining very large values.
\begin{equation}
\label{eq:update_reg}
\theta^{(i+1)} \leftarrow \theta^{(i)} + \gamma \left[\frac{\partial \log P_{\Theta}(Y_{m} = Y^{(m)})}{\partial \theta^{(i)}} - \beta \theta^{(i)} \right]
\end{equation}
here $\theta^{(i)}$ denotes the value of the parameter in the $i$-th iteration, $\theta^{i+1}$ is the value after the update, $\gamma$ is the learning rate and $\beta$ is the regularization constant.
The update rules for the different parameters are given below.
\begin{enumerate}
\item 
%\textbf{Document Vector} : 
Update rule for the Document Vector,
\begin{equation}
\label{eq:update_doc}
(\vecdi{d^{(m)}_{i}})^{(i+1)} = (\vecdi{d^{(m)}_{i}})^{(i)} + \gamma \left[ (Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c} )\vecwi{w^{(m)}_{t}} - \beta\vecdi{d^{(m)}_{i}} \right]
\end{equation}
\item 
%\textbf{Middle Word Vector} : 
Update rule for the Middle Word Vector, 
\begin{equation}
\label{eq:update_mword}
(\vecwi{w^{(m)}_{t}})^{(i+1)} = (\vecwi{w^{(m)}_{t}})^{(i)} + \gamma \left[ (Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c})) h^{(m)}_{c} - \beta\vecwi{w^{(m)}_{t}} \right]
\end{equation}
\item 
%\textbf{Context Word Vectors} : 
Update rule for the Context Word Vectors,
\begin{equation}
\label{eq:update_cword}
(\vecwi{w^{(m)}_{t+j}})^{(i+1)} = (\vecwi{w^{(m)}_{t+j}})^{(i)} + \gamma \left[(Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c}))\wgt{t+j} \vecwi{w^{(m)}_{t}} - \beta\vecwi{w^{(m)}_{t+j}} \right]
\end{equation}
\item 
%\textbf{Neural Network Weights} : 
Update rule for the Neural Network Weights,
\begin{equation}
\label{eq:update_wgt}
\wgt{t+j}^{(i+1)} = \wgt{t+j}^{(i)} + \gamma \left[ (Y^{(m)} - \sigma(\vecwi{w^{(m)}_{t}} \cdot h^{(m)}_{c}))(\vecwi{w^{(m)}_{t}} \cdot \vecwi{w^{(m)}_{t+j}}) -\beta\wgt{t+j} \right]
\end{equation}
\end{enumerate}
To learn the vectors $\matD$, $\matW$ and weights $\Lambda$ we initialize vectors in $\matD$ and $\matW$ to small random vectors the weight vector $\Lambda$ to all ones. We iterate through the training data for a fixed number of epochs, making the appropriate updates using Eqs.~\ref{eq:update_doc}, ~\ref{eq:update_mword}, ~\ref{eq:update_cword} and ~\ref{eq:update_wgt}. For each training sequence we make one update to the document vector, $2c$ updates for the context word vectors in the sequence and the neural network weights, where $c$ is the window length we consider while training. The complete algorithm for learning document and word vectors using our model is described in Algorithm~\ref{alg:doc_embeddings}.

\begin{algorithm}[h!]
\begin{algorithmic}[1]
 \State \textbf{Input: } $\setD$, $k$, $c$, $n$, $\beta$, $\gamma$, $epochs$
 \State \textbf{Output: } Document Vectors $\matD$, Word Vectors $\matW$ 
 \State $\setW \gets Extractfrom(\setD)$ 
 \State $\matD \gets random(\mathbb{R}^{k \times |\setD|})$
 \State $\matW \gets random(\mathbb{R}^{k \times |\setW|})$
 \State $\traindata \gets Extract from (\setD, c, n)$ \Comment{$|\traindata| = M + nM$}
 \State $\Lambda \gets \mathbf{1}^{2c}$ \Comment{$2c$-sized vector of 1s}
 \While{$epochs \geq 1$}
 \ForAll{$\{d_{i}, w_{t-c}, \ldots, w_{t+c}, Y\} \in \traindata$}
  \State $h_{c} \gets \vecdi{d_{i}} + \wgt{t-c}\vecwi{w_{t-c}} + \ldots + \wgt{t+c}\vecwi{w_{t+c}}$
  \State $\vecdi{d_{i}} \gets \vecdi{d_{i}} + \gamma \left[ (Y - \sigma(\vecwi{w_{t}} \cdot h_{c}))\vecwi{w_{t}} - \beta\vecdi{d_{i}} \right]$
  \State $\vecwi{w_{t}} \gets \vecwi{w_{t}} + \gamma \left[ (Y - \sigma(\vecwi{w_{t}} \cdot h_{c})) h_{c} - \beta\vecwi{w_{t}} \right]$
  \ForAll{ $j$ $\in$ $\{t-c, \ldots, t-1, t+1, \ldots, t+c \}$} 
	\State $\vecwi{w_{t+j}} \gets \vecwi{w_{t+j}} + \gamma \left[(Y - \sigma(\vecwi{w_{t}} \cdot h_{c}))\wgt{t+j} \vecwi{w_{t}} - \beta\vecwi{w_{t+j}} \right]$
  	\State $\wgt{t+j} \gets \wgt{t+j} + \gamma \left[ (Y - \sigma(\vecwi{w_{t}} \cdot h_{c}))(\vecwi{w_{t}} \cdot \vecwi{w_{t+j}}) -\beta\wgt{t+j} \right]$
  \EndFor	
  \State $epochs \gets epochs - 1$
  %compute(\vecdi{d_{i}}, \vecwi{w_{t-c}}, \ldots, \vecwi{w_{t-1}}, \vecwi{w_{t+1}}, \vecwi{w_{t+c}})$
 \EndFor 
 \EndWhile
 \\
 \Return $\matD, \matW$
\end{algorithmic}
\caption{Learning Document and Word Vector Representations}
\label{alg:doc_embeddings}
\end{algorithm}

\subsubsection{Hyper-parameters of our model}
\label{sec:hp_doc}
Our model has various hyper-parameters that need to be tuned for optimum model performance and learning high quality document and word vectors. Below we describe the hyper-parameters in our model and the effect they have on learning document representations.
\begin{enumerate}
\item \textbf{Embedding Dimensionality ($k$)} : The most important hyper-parameter in our model is the size of the document and word embedding vectors, $k$. The embedding dimensionality needs to be large enough such that the document vectors can encode the different semantic topics across the corpus but should not be so large that it introduces noise in the vectors.

\item \textbf{Window  Size ($c$)} : The length of the sequence or the window size $c$, that we consider as context surrounding a word is crucial for the quality of representations learned. While a smaller window could result in the neglect of important words that surround the middle word, a large window could introduce noise in the context that can deteriorate the performance of the model.

\item \textbf{Number of Negative Samples ($n$)} : In NCE, tuning the number of negative samples introduced in the training data per positive example is important for deciding the trade-off between learning better word density distribution, with larger $n$ and lower training complexity, with smaller $n$. 

\item \textbf{Number of Epochs ($epochs$))} : The number of times we loop through the training data needs to be controlled. We would like to learn high quality representations without overfitting and thereby learning the noise in the data.

\item \textbf{Learning Rate ($\gamma$)} : The convergence of a non-convex objective being optimized using the SGD algorithm is very sensitive to
the learning rate. While a small learning rate could harm training time and convergence, a large learning rate can lead to divergence and poor parameter values.

\item \textbf{Regularization Constant ($\beta$)} : Regularization is introduced in the training objective to avoid overfitting of parameters by reducing model complexity. While a small $\beta$ may not penalize complexity enough, larger constants may inhibit the growth of parameters to their optimum values. So, this requires careful selection of the regularization constant and depends on the dataset.
\end{enumerate}
We explain the technique by which we tune these hyper-parameters later in Sec.~\ref{sec:exp_setup}.




