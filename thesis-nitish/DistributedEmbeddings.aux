\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Distributed Document Embeddings}{13}{chapter.35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:distembed}{{2}{13}{Distributed Document Embeddings}{chapter.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{13}{section.36}}
\newlabel{sec:motivation_distributed}{{2.1}{13}{Motivation}{section.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Background on Word Embeddings}{14}{section.39}}
\newlabel{sec:background_distributed}{{2.2}{14}{Background on Word Embeddings}{section.39}{}}
\citation{bengio2003neural}
\citation{mnih2013learning}
\citation{mikolov2013distributed}
\citation{collobert2011natural}
\citation{bottou2014machine}
\citation{turian2010word}
\citation{levy2014dependencybased}
\citation{bengio2003neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Neural Probabilistic Language Model (NPLM)}{15}{subsection.40}}
\newlabel{sec:bengio}{{2.2.1}{15}{Neural Probabilistic Language Model (NPLM)}{subsection.40}{}}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Bengio's Neural Network Architechture for Neural Probabilistic Language Model\relax }}{16}{figure.caption.43}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nn:bengio}{{2.1}{16}{Bengio's Neural Network Architechture for Neural Probabilistic Language Model\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Log-Linear Models : word2vec}{17}{subsection.47}}
\newlabel{sec:word2vec}{{2.2.2}{17}{Log-Linear Models : word2vec}{subsection.47}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.1}Continuous Bag-of-Words (CBOW)}{17}{subsubsection.48}}
\newlabel{sec:cbow}{{2.2.2.1}{17}{Continuous Bag-of-Words (CBOW)}{subsubsection.48}{}}
\newlabel{eq:cbow:prob}{{2.8}{17}{Continuous Bag-of-Words (CBOW)}{equation.52}{}}
\citation{morin2005hierarchical}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Continuous Bag-of-Words Model (CBOW) \leavevmode {\color  {red}\textbf  {TODO:} Add ref?}\relax }}{18}{figure.caption.49}}
\newlabel{fig:nn:cbow}{{2.2}{18}{Continuous Bag-of-Words Model (CBOW) \todo {Add ref?}\relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.2}Continuous Skip-gram}{18}{subsubsection.53}}
\newlabel{eq:skip:prob}{{2.9}{18}{Continuous Skip-gram}{equation.55}{}}
\citation{mikolov2013linguistic}
\citation{levy2014dependencybased}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Continuous Skip-gram Model \leavevmode {\color  {red}\textbf  {TODO:} Add ref?}\relax }}{19}{figure.caption.54}}
\newlabel{fig:nn:skip}{{2.3}{19}{Continuous Skip-gram Model \todo {Add ref?}\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.3}Dependency-based Word Embeddings}{19}{subsubsection.56}}
\citation{mitchell2010composition}
\citation{zanzotto2010estimating}
\citation{yessenalina2011compositional}
\citation{grefenstette2013multi}
\citation{mikolov2013distributed}
\citation{socher2013recursive}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Dependency-based context extraction example \leavevmode {\color  {red}\textbf  {TODO:} Add ref?}\relax }}{20}{figure.caption.57}}
\newlabel{fig:dep:context}{{2.4}{20}{Dependency-based context extraction example \todo {Add ref?}\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Document Embeddings}{20}{section.58}}
\newlabel{sec:document_embeddings}{{2.3}{20}{Document Embeddings}{section.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Problem Setup}{21}{subsection.61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Our Model}{22}{subsection.62}}
\newlabel{sec:docem_ourmodel}{{2.3.2}{22}{Our Model}{subsection.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces GloDETC : Neural Network Archtichture \leavevmode {\color  {red}\textbf  {TODO:} Change figure}\relax }}{23}{figure.caption.66}}
\newlabel{fig:nn:archi}{{2.5}{23}{GloDETC : Neural Network Archtichture \todo {Change figure}\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.1}Projection Layer (Context Representation)}{23}{subsubsection.67}}
\newlabel{eq:hidden_vec}{{2.10}{24}{Projection Layer (Context Representation)}{equation.68}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.2}Estimating Prediction Probability}{24}{subsubsection.69}}
\newlabel{eq:nn_score}{{2.11}{24}{Estimating Prediction Probability}{equation.71}{}}
\newlabel{eq:soft_prob}{{2.12}{24}{Estimating Prediction Probability}{equation.73}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.3}Training Objective}{24}{subsubsection.74}}
\citation{morin2005hierarchical}
\citation{bengio2003quick}
\citation{bengio2008adaptive}
\citation{gutmann2012noise}
\citation{mnih2012fast}
\citation{mnih2013learning}
\newlabel{eq:paramter_argmax}{{2.13}{25}{Training Objective}{equation.75}{}}
\newlabel{eq:training_objective}{{2.14}{25}{Training Objective}{equation.76}{}}
\newlabel{eq:update_theta}{{2.15}{25}{Training Objective}{equation.77}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.4}Noise Contrastive Estimation (NCE)}{25}{subsubsection.78}}
\newlabel{eq:label1}{{2.16}{26}{Noise Contrastive Estimation (NCE)}{equation.79}{}}
\newlabel{eq:label0}{{2.17}{27}{Noise Contrastive Estimation (NCE)}{equation.80}{}}
\newlabel{eq:prob_y}{{2.18}{27}{Noise Contrastive Estimation (NCE)}{equation.81}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.5}New Training Objective}{27}{subsubsection.82}}
\newlabel{eq:new_argmax}{{2.19}{27}{New Training Objective}{equation.83}{}}
\newlabel{eq:new_training_objective}{{2.20}{27}{New Training Objective}{equation.84}{}}
\newlabel{eq:log_P}{{2.21}{27}{New Training Objective}{equation.85}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.6}Parameter Estimation}{27}{subsubsection.86}}
\newlabel{sec:para_esti_doc}{{2.3.2.6}{27}{Parameter Estimation}{subsubsection.86}{}}
\newlabel{eq:partial_theta}{{2.25}{28}{Parameter Estimation}{equation.90}{}}
\newlabel{eq:partial_doc}{{2.26}{28}{Parameter Estimation}{equation.91}{}}
\newlabel{eq:partial_w_t}{{2.27}{28}{Parameter Estimation}{equation.92}{}}
\newlabel{eq:partial_w_t-k}{{2.28}{28}{Parameter Estimation}{equation.93}{}}
\newlabel{eq:partial_wgt_t-k}{{2.29}{28}{Parameter Estimation}{equation.94}{}}
\newlabel{eq:grad_doc}{{2.30}{29}{Parameter Estimation}{equation.96}{}}
\newlabel{eq:grad_mword}{{2.31}{29}{Parameter Estimation}{equation.98}{}}
\newlabel{eq:grad_cword}{{2.32}{29}{Parameter Estimation}{equation.100}{}}
\newlabel{eq:grad_wgt}{{2.33}{29}{Parameter Estimation}{equation.102}{}}
\newlabel{eq:update_reg}{{2.34}{29}{Parameter Estimation}{equation.103}{}}
\newlabel{eq:update_doc}{{2.35}{29}{Parameter Estimation}{equation.105}{}}
\newlabel{eq:update_mword}{{2.36}{30}{Parameter Estimation}{equation.107}{}}
\newlabel{eq:update_cword}{{2.37}{30}{Parameter Estimation}{equation.109}{}}
\newlabel{eq:update_wgt}{{2.38}{30}{Parameter Estimation}{equation.111}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.7}Hyper-parameters}{30}{subsubsection.112}}
\@setckpt{DistributedEmbeddings}{
\setcounter{page}{32}
\setcounter{equation}{38}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{subsubsection}{7}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{float@type}{16}
\setcounter{algorithm}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{lstnumber}{1}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{36}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{29}
\setcounter{AM@survey}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{example}{0}
\setcounter{definition}{0}
\setcounter{proposition}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{3}
}
