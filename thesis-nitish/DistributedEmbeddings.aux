\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Distributed Document Representations}{19}{chapter.34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:distembed}{{3}{19}{Distributed Document Representations}{chapter.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Need for Distributed Word Representations}{19}{section.35}}
\newlabel{sec:motivation_distributed}{{3.1}{19}{Need for Distributed Word Representations}{section.35}{}}
\citation{bengio2003neural,mnih2013learning,mikolov2013distributed,collobert2011natural,bottou2014machine,turian2010word,levy2014dependencybased}
\citation{bengio2003neural}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Background on Word Embeddings}{20}{section.38}}
\newlabel{sec:background_distributed}{{3.2}{20}{Background on Word Embeddings}{section.38}{}}
\citation{bengio2003neural}
\citation{bengio2003neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Neural Probabilistic Language Model}{21}{subsection.39}}
\newlabel{sec:bengio}{{3.2.1}{21}{Neural Probabilistic Language Model}{subsection.39}{}}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural Network Architecture for Neural Probabilistic Language Model (NPLM). \cite  {bengio2003neural}\relax }}{22}{figure.caption.42}}
\newlabel{fig:nn:bengio}{{3.1}{22}{Neural Network Architecture for Neural Probabilistic Language Model (NPLM). \cite {bengio2003neural}\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Log-Linear Models}{22}{subsection.46}}
\newlabel{sec:word2vec}{{3.2.2}{22}{Log-Linear Models}{subsection.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.1}Continuous Bag-of-Words Model}{23}{subsubsection.47}}
\newlabel{sec:cbow}{{3.2.2.1}{23}{Continuous Bag-of-Words Model}{subsubsection.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Continuous Bag-of-Words Model (CBOW). Ref. \cite  {mikolov2013efficient} \relax }}{23}{figure.caption.48}}
\newlabel{fig:nn:cbow}{{3.2}{23}{Continuous Bag-of-Words Model (CBOW). Ref. \cite {mikolov2013efficient} \relax }{figure.caption.48}{}}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{morin2005hierarchical}
\newlabel{eq:cbow:prob}{{3.8}{24}{Continuous Bag-of-Words Model}{equation.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.2}Continuous Skip-gram Model}{24}{subsubsection.52}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Continuous Skip-gram Model. Ref. \cite  {mikolov2013efficient}\relax }}{24}{figure.caption.53}}
\newlabel{fig:nn:skip}{{3.3}{24}{Continuous Skip-gram Model. Ref. \cite {mikolov2013efficient}\relax }{figure.caption.53}{}}
\newlabel{eq:skip:prob}{{3.9}{24}{Continuous Skip-gram Model}{equation.54}{}}
\citation{mikolov2013linguistic}
\citation{levy2014dependencybased}
\citation{levy2014dependencybased}
\citation{levy2014dependencybased}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.3}Dependency-based Word Embeddings}{25}{subsubsection.55}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Example for Dependency-based context extraction from \cite  {levy2014dependencybased}\relax }}{25}{figure.caption.56}}
\newlabel{fig:dep:context}{{3.4}{25}{Example for Dependency-based context extraction from \cite {levy2014dependencybased}\relax }{figure.caption.56}{}}
\citation{mitchell2010composition,zanzotto2010estimating,yessenalina2011compositional,grefenstette2013multi,mikolov2013distributed}
\citation{socher2013recursive}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Document Representation}{26}{section.57}}
\newlabel{sec:document_embeddings}{{3.3}{26}{Document Representation}{section.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Problem Setup}{27}{subsection.60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Our Model}{27}{subsection.61}}
\newlabel{sec:docem_ourmodel}{{3.3.2}{27}{Our Model}{subsection.61}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.1}Context Representation}{28}{subsubsection.66}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Neural network Architecture for our model\relax }}{29}{figure.caption.65}}
\newlabel{fig:nn:archi}{{3.5}{29}{Neural network Architecture for our model\relax }{figure.caption.65}{}}
\newlabel{eq:hidden_vec}{{3.10}{29}{Context Representation}{equation.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.2}Estimating Prediction Probability}{29}{subsubsection.68}}
\newlabel{eq:nn_score}{{3.11}{29}{Estimating Prediction Probability}{equation.70}{}}
\citation{morin2005hierarchical}
\citation{bengio2003quick,bengio2008adaptive}
\newlabel{eq:soft_prob}{{3.12}{30}{Estimating Prediction Probability}{equation.72}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.3}Learning Objective}{30}{subsubsection.73}}
\newlabel{eq:paramter_argmax}{{3.13}{30}{Learning Objective}{equation.74}{}}
\newlabel{eq:training_objective}{{3.14}{30}{Learning Objective}{equation.75}{}}
\newlabel{eq:update_theta}{{3.15}{30}{Learning Objective}{equation.76}{}}
\citation{gutmann2012noise}
\citation{mnih2012fast}
\citation{mnih2013learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.4}Noise Contrastive Estimation}{31}{subsubsection.77}}
\newlabel{eq:label1}{{3.16}{32}{Noise Contrastive Estimation}{equation.78}{}}
\newlabel{eq:label0}{{3.17}{32}{Noise Contrastive Estimation}{equation.79}{}}
\newlabel{eq:prob_y}{{3.18}{32}{Noise Contrastive Estimation}{equation.80}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.5}Learning Objective using NCE}{32}{subsubsection.81}}
\newlabel{eq:new_argmax}{{3.19}{32}{Learning Objective using NCE}{equation.82}{}}
\newlabel{eq:new_training_objective}{{3.20}{33}{Learning Objective using NCE}{equation.83}{}}
\newlabel{eq:log_P}{{3.21}{33}{Learning Objective using NCE}{equation.84}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.6}Parameter Estimation}{33}{subsubsection.85}}
\newlabel{sec:para_esti_doc}{{3.3.2.6}{33}{Parameter Estimation}{subsubsection.85}{}}
\newlabel{eq:partial_theta}{{3.25}{33}{Parameter Estimation}{equation.89}{}}
\newlabel{eq:partial_doc}{{3.26}{34}{Parameter Estimation}{equation.90}{}}
\newlabel{eq:partial_w_t}{{3.27}{34}{Parameter Estimation}{equation.91}{}}
\newlabel{eq:partial_w_t-k}{{3.28}{34}{Parameter Estimation}{equation.92}{}}
\newlabel{eq:partial_wgt_t-k}{{3.29}{34}{Parameter Estimation}{equation.93}{}}
\newlabel{eq:grad_doc}{{3.30}{34}{Parameter Estimation}{equation.95}{}}
\newlabel{eq:grad_mword}{{3.31}{34}{Parameter Estimation}{equation.97}{}}
\newlabel{eq:grad_cword}{{3.32}{34}{Parameter Estimation}{equation.99}{}}
\newlabel{eq:grad_wgt}{{3.33}{34}{Parameter Estimation}{equation.101}{}}
\newlabel{eq:update_reg}{{3.34}{35}{Parameter Estimation}{equation.102}{}}
\newlabel{eq:update_doc}{{3.35}{35}{Parameter Estimation}{equation.104}{}}
\newlabel{eq:update_mword}{{3.36}{35}{Parameter Estimation}{equation.106}{}}
\newlabel{eq:update_cword}{{3.37}{35}{Parameter Estimation}{equation.108}{}}
\newlabel{eq:update_wgt}{{3.38}{35}{Parameter Estimation}{equation.110}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Learning Document and Word Vector Representations\relax }}{36}{algorithm.111}}
\newlabel{alg:doc_embeddings}{{1}{36}{Learning Document and Word Vector Representations\relax }{algorithm.111}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.7}Hyper-parameters of our model}{36}{subsubsection.112}}
\newlabel{sec:hp_doc}{{3.3.2.7}{36}{Hyper-parameters of our model}{subsubsection.112}{}}
\@setckpt{DistributedEmbeddings}{
\setcounter{page}{38}
\setcounter{equation}{38}
\setcounter{enumi}{6}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{subsubsection}{7}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{float@type}{16}
\setcounter{algorithm}{1}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ALG@line}{20}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{lstnumber}{1}
\setcounter{cp@cnt}{0}
\setcounter{cp@tempcnt}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{23}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{16}
\setcounter{AM@survey}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{example}{0}
\setcounter{definition}{0}
\setcounter{proposition}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{3}
}
