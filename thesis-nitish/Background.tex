\chapter{Background}
\label{chapter:Background}
Systems using WordNet as the underlying ontology often suffer because of the fine-grained nature of the sense inventory. With more and more applications using WordNet, clustering word senses such that the application developer has the control over the granularity becomes very important.

\paragraph{}
One of the earliest attempt at coarsening of a Machine Readable dictionaries was made by \citep{Dolan:1994}. 
They attempted to discover sense similarities between senses of Longman's Dictionary of Contemporary English(LDOCE) using multiple heuristics based on a variety of information about a sense's meaning.
%\cite{ChenC98} also presents a LDOCE coarsening algorithm based on information retrieval techniques. % Not useful in context of WordNet coarsening ????

\paragraph{}
A wide number of manual and automatic techniques have been proposed since then for clustering sense inventories and creating mappings across sense inventories of different granularities. This chapter talks about the previous attempts to generate coarse senses for words, and the different ideas involved in the same.

\section{Clustering WordNet Senses}
We discuss the approaches proposed in literature to cluster sense inventories in context of WordNet. Ideas to cluster senses can be broadly divided in the following categories :
\begin{itemize}
\item Merging senses based on ontology structure
\item Clustering based on word sense similarity estimated from external corpora
\item Exploiting Disagreements : between WSD systems or between human annotators
\item Translational equivalences of senses in other languages
\item Manually-annotated or automatically constructed mappings to coarser grained sense inventories
\item Clustering senses using supervision
\end{itemize}

\subsection{Merging senses based on ontology structure}
\citep{peters1998automatic} suggest clustering of two word senses based on a wide variety of structural cues from the ontology structure. They cluster senses which are connected by ontology based relations like  \textit{twins}\footnote{Two synsets which share more than one word in their synonym list}, \textit{autohyponymy}\footnote{If one sense is a direct descendant of other in ontology}, 
\textit{sisters}\footnote{Word senses that share the same hypernym : The sister relation is not limited to two senses, but can also occur between three or more senses of the same word. Sometimes, a particular word exhibits more than one type of sister relation}, 
\textit{cousins}\footnote{Node pairs whose hyponyms exhibit a specific relation to each other : identified and listed by lexicographers in WordNet 1.5} etc.
\citep{Mihalcea01ez.wordnet:principles} extended this idea and proposed six semantic principles to merge synsets and probabilistic principles to drop infrequent synsets.
Some interesting principles include merging synsets sharing a \textit{pertainym}, \textit{antonym} or sharing same \textit{verb group}\footnote{Verb Groups are manually determined by lexicographers}.
This was the first attempt to group synsets instead of word senses.

\paragraph{}
A number of synset similarity measures based on the WordNet structure have also been proposed in literature like 
Path Based Similarity Measures by \citep{WuPalmer:1994}, \citep{LCH:1998}, Information Content Based Measures by \citep{Resnik:1995}, \citep{JCN:1997}, \citep{Lin:1998} and Gloss Based Heuristics by \citep{Lesk:1986},\citep{Banerjee:2002}. We discuss these similarity measures in detail in Section \ref{section:similarityMeasures}.
Though these measures have not been used directly for WordNet sense clustering, they have motivated researchers in the NLP community to make full use of the WordNet structure to capture similarity between senses.

\subsection{Clustering based on Word Sense similarity estimated from External Corpora}
For estimating similarity between words, many corpus oriented attempts have been made like \citep{Pereira:93a}, \citep{Lin:1998}, \citep{kolb2008disco} and \citep{agirre2009study}. The problem with these approaches is that they are not able to handle the polysemous nature of words; however the dearth of sense annotated corpora prevents most of these methods to be used effectively in computing word sense similarities.

\paragraph{}
\citep{agirre2003clustering} collected contexts for a polysemous word from manually sense-tagged corpora and by using instances of a polysemous word's monosemous relatives(i.e. single-sense synsets related by hypernym, hyponym or any other relation of WordNet) from large untagged corpus and from web. While related senses may not have a lot of shared contexts directly, because of lack of sense annotated data, they may have semantic associations with the same subset of words that share similar distributional contexts with the target word. By using distributional neighbours from raw text, the method avoids the data sparsity problem.

\paragraph{}
On similar lines is the approach by \citep{mccarthy2006relating}. They use a combination of word-to-word distributional similarity combined with the JCN WordNet based similarity measure \citep{JCN:1997}. They introduce a softer notation of sense relatedness which allows the user to control the granularity for the application in hand.

\subsection{Exploiting Disagreements : between WSD Systems or between Human Annotators}
This is a totally different family of approaches. 
The central idea involved here is that whenever WSD systems or human annotators get confused while disambiguation, the senses they mark as answers are semantically related to the correct answer.
\begin{example}
Consider the following noun senses of the word \textit{bass} :
\begin{itemize}
\item bass (the lowest part of the musical range)
\item bass, bass part (the lowest part in polyphonic music)
\item bass, basso (an adult male singer with the lowest voice)
\item sea bass, bass (the lean flesh of a saltwater fish of the family Serranidae)
\item freshwater bass, bass (any of various North American freshwater fish with lean flesh (especially of the genus Micropterus))
\item bass, bass voice, basso (the lowest adult male singing voice)
\item bass (the member with the lowest range of a family of musical instruments)
\item bass (nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes)
\end{itemize}
\end{example}

It is unlikely that a human annotator mistags the musical sense of \textit{bass} with its fish sense. Similar results are expected from a good WSD system as well.

\paragraph{}
\citep{chklovski2003exploiting} derives confusion matrices exploiting the disagreements between human annotators and uses the same to generate coarse sense clusters. On the other hand, \citep{agirre2003clustering} uses the freely available outputs of the WSD systems that participated in Senseval-2 \citep{Edmonds:2001} to construct the confusion matrices between word senses and then cluster them using hierarchial agglomerative clustering. Though promising, these techniques are severely limited by the amount of available manually sense-tagged data and the performance of the WSD systems.

\subsection{Translational Equivalences of Senses in other languages}
\citep{chugur2002polysemy} constructed similarity matrices for Senseval-2 \citep{Edmonds:2001} words using \textbf{translation equivalences} in 4 languages, a method proposed by \citep{resnik1999distinguishing}.
The principle involved can be summed as : \textit{two word senses are deemed similar if they are often translated with the same word in a given context}. Using more than one languages allows the systems to cover as many word sense distinctions as possible. \citep{agirre2003clustering} uses the similarity matrices provided by \citep{chugur2002polysemy} and report resulting hierarchial clusters.

With the advent of WordNets being developed in multiple languages\footnote{GlobalWordNet lists the WordNets available in the public domains : \url{http://www.globalwordnet.org/gwa/wordnet_table.html}} as well as multilingual ontologies like BabelNet \citep{NavigliPonzetto:12aij}, this seems a promising area which can help in coarsening of senses.


\subsection{Manually-annotated or automatically constructed mappings to coarser grained sense inventories}
Mapping WordNet to other inventories either manually or automatically to generate coarse senses has also been under the lime light of many researchers in the NLP community. When the different WordNet senses map to same sense in the other ontology via manual mapping or automatic mapping, it is expected that the senses must have been semantically close. The underlying assumption being that the automatic mapping is able to capture the semantic similarity between the concepts in both the ontologies with high efficacy.

The attempts made in this vein include mapping between WordNet and Hector Lexicon \citep{palmer2007making}, mapping between WordNet and PropBank \citep{palmer2004different} and mapping WordNet to Levin Classes \citep{levin1993english} \citep{palmer2007making}. Most of these mappings are not complete in both directions, which hampers their utility.

The automatic approach presented by \citep{Navigli06meaningfulclustering} for mapping between sense inventories, WordNet to Oxford English Dictionary to be precise, is an elegant approach exploiting similarities in gloss definition and structured relationships in the two sense inventories. The approach can be extended to discover more semantic relationships in WordNet by using the ontology structure of the ontology mapped to WordNet.

\subsection{Clustering senses using supervision}
One of the earliest attempt to cluster senses using supervision was proposed by \citep{snow07mergesense}. They train a Support Vector Machine \citep{vapnikSVM:95} over a wide variety of features derived from WordNet and other lexical resources, whose predictions serve as a distance measure between synsets. Further, for the purpose of sense clustering they assume a zero sense similarity score between synsets with no intersecting words. They cluster synsets using average link agglomerative clustering and the synset similarity model learnt. While merging synsets to construct a coarse taxonomy, they retain only the hypernym ancestry of the sense with the highest frequency in SemCor \citep{SemCor}. They add every other relationship to the new merged sense as long as the acyclic nature of the relations is conserved.

\section{Evolution of Evaluation Frameworks}
We would like to highlight here the different frameworks used in literature for evaluation of sense clustering problem. Early systems studied the quality of clusters of word senses by studying polysemy degree of the text \citep{Mihalcea01ez.wordnet:principles} or by measuring the entropy and purity of the clusters obtained \citep{agirre2003clustering}.

Another idea is to compare the clustering obtained against a manually sense clustered dataset as done by \citep{chklovski2003exploiting}. This can be done by treating clustering as a pairwise classification task and reporting the F-Score for the classification task. The problem with this approach of evaluation is that in clustering applications often the number of pairs in a cluster is relatively small. This imbalance could lead to understatement of pairwise similarity and can be avoided by using FScore for both the classes as performance evaluators.

The recent line of thought in evaluation is to go for a task based evaluation. \citep{mccarthy2006relating} studied the performance of first sense heuristics in the Senseval-2 English Lexical Sample Task \citep{Senseval2LexicalSampleTask}. \citep{Navigli06meaningfulclustering} and \citep{snow07mergesense} assess the effect of the automatic sense clustering on the three best-ranking WSD systems of the English all-words task at Senseval-3 \citep{Senseval3AllWordsTask}. Since the main reason for building a clustering of WordNet senses is to make WSD a feasible task, studying the performance of WSD systems on coarse sense inventories produced seems a well founded approach.

\section{Broad Overview of our Approach}
Our approach closely resembles \citep{snow07mergesense} as far as supervised learning of the synset similarity is concerned. But to learn synset similarity of synset pairs which don't share a word, instead of giving them zero similarity, we learn it using a variant of the SimRank framework \citep{Jeh02simrank}. 

Also, \citep{snow07mergesense} proposes to modify the WordNet ontology structure to produce a coarse version of WordNet; however we argue on the lines of \citep{mccarthy2006relating} that we should relate senses as a matter of degree to permit a softer notion of relationships between senses compared to fixed groupings so that the granularity can be varied according to the needs of the application.